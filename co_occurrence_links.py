"""
ä»Keywords_Processedæ–‡ä»¶ç”Ÿæˆå…³é”®è¯å…±ç°é“¾æ¥æ–‡ä»¶
åŸºäºä¸“åˆ©ä¸­å…³é”®è¯çš„å…±ç°å…³ç³»æ„å»ºç½‘ç»œè¾¹æ•°æ®
"""

import pandas as pd
import numpy as np
from itertools import combinations
from collections import Counter
from datetime import datetime
import ast
from tqdm import tqdm
import os

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

# é…ç½®
DATA_DIR = 'F:/WLJ/Weak-Link-Effect-Research/data'

# è¾“å…¥æ–‡ä»¶è·¯å¾„
KEYWORDS_FILE = os.path.join(DATA_DIR, 'Keywords_Processed_20250903_211104.csv')

print("=" * 80)
print("ç”Ÿæˆå…³é”®è¯å…±ç°é“¾æ¥æ–‡ä»¶")
print("=" * 80)

# 1. è¯»å–å…³é”®è¯æ•°æ®
print("\n[1] è¯»å–å…³é”®è¯æ•°æ®...")
df_keywords = pd.read_csv(KEYWORDS_FILE)
print(f"  æ€»è®°å½•æ•°: {len(df_keywords):,}")
print(f"  å¹´ä»½èŒƒå›´: {df_keywords['earliest_publn_year'].min()} - {df_keywords['earliest_publn_year'].max()}")

# 2. è§£æå…³é”®è¯å¹¶ç”Ÿæˆå…±ç°å…³ç³»
print("\n[2] æå–å…³é”®è¯å…±ç°å…³ç³»...")

co_occurrence_records = []
keyword_stats = Counter()
year_stats = Counter()

for idx, row in tqdm(df_keywords.iterrows(), total=len(df_keywords), desc="å¤„ç†ä¸“åˆ©"):
    # è·å–åŸºæœ¬ä¿¡æ¯
    appln_id = row['appln_id']
    year = row['earliest_publn_year']
    
    # è§£æå…³é”®è¯åˆ—è¡¨
    try:
        # å°è¯•ç”¨ast.literal_evalè§£æï¼ˆå¦‚æœæ˜¯å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨ï¼‰
        keywords_str = row['keywords']
        if pd.isna(keywords_str) or keywords_str == '[]':
            continue
        keywords = ast.literal_eval(keywords_str)
    except:
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        try:
            keywords = row['keywords'].split(',') if isinstance(row['keywords'], str) else []
            # æ¸…ç†å…³é”®è¯ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ï¼‰
            keywords = [kw.strip() for kw in keywords if kw.strip()]
        except:
            continue
    
    # è·³è¿‡ç©ºåˆ—è¡¨æˆ–å•ä¸ªå…³é”®è¯çš„æƒ…å†µ
    if len(keywords) < 2:
        continue
    
    # ç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
    for kw in keywords:
        keyword_stats[kw] += 1
    
    # ç»Ÿè®¡å¹´ä»½
    year_stats[year] += 1
    
    # ç”Ÿæˆå…³é”®è¯å¯¹ï¼ˆç»„åˆï¼‰- ç¡®ä¿é¡ºåºä¸€è‡´æ€§
    for kw1, kw2 in combinations(sorted(keywords), 2):
        co_occurrence_records.append({
            'appln_id': appln_id,
            'year': year,
            'keyword1': kw1,
            'keyword2': kw2
        })

print(f"\n  æ‰¾åˆ° {len(co_occurrence_records):,} ä¸ªå…±ç°å…³ç³»")
print(f"  æ¶‰åŠ {len(keyword_stats):,} ä¸ªä¸åŒå…³é”®è¯")

# 3. è½¬æ¢ä¸ºDataFrameå¹¶ç»Ÿè®¡
print("\n[3] ç»Ÿè®¡å…±ç°é¢‘æ¬¡...")
df_cooc = pd.DataFrame(co_occurrence_records)

# æŒ‰å¹´ä»½å’Œå…³é”®è¯å¯¹åˆ†ç»„ï¼Œç»Ÿè®¡å…±ç°æ¬¡æ•°
df_links = df_cooc.groupby(['year', 'keyword1', 'keyword2']).agg({
    'appln_id': 'count'  # ç»Ÿè®¡å…±ç°æ¬¡æ•°
}).reset_index()

# é‡å‘½ååˆ—
df_links.rename(columns={'appln_id': 'co_occurrences'}, inplace=True)

# æ·»åŠ è¾¹çš„ID
df_links.insert(0, 'edge_id', range(1, len(df_links) + 1))

# 4. è®¡ç®—é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
print("\n[4] è®¡ç®—è¾¹çš„ç»Ÿè®¡ä¿¡æ¯...")

# ä¸ºæ¯æ¡è¾¹è®¡ç®—å…³é”®è¯çš„æ€»å‡ºç°æ¬¡æ•°ï¼ˆç”¨äºåç»­åˆ†æï¼‰
df_links['keyword1_freq'] = df_links['keyword1'].map(keyword_stats)
df_links['keyword2_freq'] = df_links['keyword2'].map(keyword_stats)

# è®¡ç®—è¾¹çš„æƒé‡ï¼ˆå¯ä»¥åŸºäºå…±ç°æ¬¡æ•°å’Œå…³é”®è¯é¢‘ç‡ï¼‰
# ä½¿ç”¨Jaccardç›¸ä¼¼åº¦çš„æ€æƒ³
df_links['edge_weight'] = df_links['co_occurrences'] / (
    df_links['keyword1_freq'] + df_links['keyword2_freq'] - df_links['co_occurrences']
)

# 5. æ•°æ®æ¸…ç†å’Œæ’åº
print("\n[5] æ•°æ®æ¸…ç†å’Œæ’åº...")

# æŒ‰å¹´ä»½å’Œå…±ç°æ¬¡æ•°æ’åº
df_links = df_links.sort_values(['year', 'co_occurrences'], ascending=[True, False])

# é€‰æ‹©è¦ä¿å­˜çš„åˆ—
columns_to_save = [
    'edge_id', 
    'year', 
    'keyword1', 
    'keyword2', 
    'co_occurrences',
    'edge_weight',
    'keyword1_freq',
    'keyword2_freq'
]

df_final = df_links[columns_to_save].copy()

# 6. ç»Ÿè®¡ä¿¡æ¯è¾“å‡º
print("\n" + "=" * 80)
print("å…±ç°é“¾æ¥ç»Ÿè®¡:")
print("=" * 80)
print(f"  æ€»è¾¹æ•°: {len(df_final):,}")
print(f"  å¹´ä»½èŒƒå›´: {df_final['year'].min()} - {df_final['year'].max()}")
print(f"  å¹³å‡å…±ç°æ¬¡æ•°: {df_final['co_occurrences'].mean():.2f}")
print(f"  æœ€å¤§å…±ç°æ¬¡æ•°: {df_final['co_occurrences'].max()}")
print(f"  æœ€å°å…±ç°æ¬¡æ•°: {df_final['co_occurrences'].min()}")

# å„å¹´ä»½è¾¹æ•°ç»Ÿè®¡
print("\nå„å¹´ä»½ç»Ÿè®¡:")
year_edge_stats = df_final.groupby('year').agg({
    'edge_id': 'count',
    'co_occurrences': ['sum', 'mean', 'max']
}).round(2)
year_edge_stats.columns = ['è¾¹æ•°', 'æ€»å…±ç°æ¬¡æ•°', 'å¹³å‡å…±ç°', 'æœ€å¤§å…±ç°']
print(year_edge_stats.head(10))

# Topå…³é”®è¯å¯¹
print("\nTop 10 æœ€é¢‘ç¹çš„å…³é”®è¯å¯¹:")
top_pairs = df_final.nlargest(10, 'co_occurrences')[
    ['keyword1', 'keyword2', 'co_occurrences', 'year']
]
for idx, row in top_pairs.iterrows():
    print(f"  {row['keyword1']} <-> {row['keyword2']}: {row['co_occurrences']} æ¬¡ ({row['year']}å¹´)")

# 7. ä¿å­˜æ–‡ä»¶
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"keyword_co_occurrence_links_{timestamp}.csv"
output_path = os.path.join(DATA_DIR, output_filename)

df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n" + "=" * 80)
print(f"âœ… å…³é”®è¯å…±ç°é“¾æ¥æ–‡ä»¶å·²ç”Ÿæˆ!")
print(f"   æ–‡ä»¶å: {output_filename}")
print(f"   è·¯å¾„: {output_path}")
print(f"   æ€»è¾¹æ•°: {len(df_final):,}")
print("=" * 80)

# 8. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
report_filename = f"keyword_co_occurrence_report_{timestamp}.txt"
report_path = os.path.join(DATA_DIR, report_filename)

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("å…³é”®è¯å…±ç°é“¾æ¥ç”ŸæˆæŠ¥å‘Š\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"æºæ–‡ä»¶: {KEYWORDS_FILE}\n\n")
    
    f.write("æ•°æ®æ¦‚è§ˆ:\n")
    f.write(f"  è¾“å…¥ä¸“åˆ©æ•°: {len(df_keywords):,}\n")
    f.write(f"  ç”Ÿæˆå…±ç°å¯¹æ•°: {len(co_occurrence_records):,}\n")
    f.write(f"  èšåˆåè¾¹æ•°: {len(df_final):,}\n")
    f.write(f"  æ¶‰åŠå…³é”®è¯æ•°: {len(keyword_stats):,}\n\n")
    
    f.write("æ—¶é—´åˆ†å¸ƒ:\n")
    f.write(f"  å¹´ä»½èŒƒå›´: {df_final['year'].min()} - {df_final['year'].max()}\n")
    f.write(f"  å¹´ä»½æ•°é‡: {df_final['year'].nunique()}\n\n")
    
    f.write("å…±ç°ç»Ÿè®¡:\n")
    f.write(f"  å¹³å‡å…±ç°æ¬¡æ•°: {df_final['co_occurrences'].mean():.2f}\n")
    f.write(f"  ä¸­ä½æ•°: {df_final['co_occurrences'].median():.0f}\n")
    f.write(f"  æ ‡å‡†å·®: {df_final['co_occurrences'].std():.2f}\n")
    f.write(f"  æœ€å°å€¼: {df_final['co_occurrences'].min()}\n")
    f.write(f"  æœ€å¤§å€¼: {df_final['co_occurrences'].max()}\n\n")
    
    f.write("è¾¹æƒé‡åˆ†å¸ƒ:\n")
    weight_stats = df_final['edge_weight'].describe()
    f.write(str(weight_stats))
    f.write("\n\n")
    
    f.write("å„å¹´ä»½è¯¦ç»†ç»Ÿè®¡:\n")
    f.write(str(year_edge_stats))
    f.write("\n\n")
    
    f.write("Top 20 é«˜é¢‘å…³é”®è¯:\n")
    for kw, count in keyword_stats.most_common(20):
        f.write(f"  {kw}: {count} æ¬¡\n")
    f.write("\n")
    
    f.write("Top 20 é«˜é¢‘å…±ç°å¯¹:\n")
    top20_pairs = df_final.nlargest(20, 'co_occurrences')
    for idx, row in top20_pairs.iterrows():
        f.write(f"  {row['keyword1']} <-> {row['keyword2']}: ")
        f.write(f"{row['co_occurrences']} æ¬¡ (æƒé‡: {row['edge_weight']:.4f})\n")
    
    f.write("\næ•°æ®å­—æ®µè¯´æ˜:\n")
    f.write("  edge_id: è¾¹çš„å”¯ä¸€æ ‡è¯†ç¬¦\n")
    f.write("  year: å…±ç°å‘ç”Ÿçš„å¹´ä»½\n")
    f.write("  keyword1, keyword2: å…±ç°çš„ä¸¤ä¸ªå…³é”®è¯\n")
    f.write("  co_occurrences: å…±ç°æ¬¡æ•°ï¼ˆåœ¨å¤šå°‘ä¸ªä¸“åˆ©ä¸­åŒæ—¶å‡ºç°ï¼‰\n")
    f.write("  edge_weight: è¾¹æƒé‡ï¼ˆåŸºäºJaccardç›¸ä¼¼åº¦ï¼‰\n")
    f.write("  keyword1_freq, keyword2_freq: å„å…³é”®è¯çš„æ€»å‡ºç°é¢‘ç‡\n")

print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}")

# 9. ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
print("\nç”Ÿæˆå…±ç°åˆ†å¸ƒå›¾...")
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. å…±ç°æ¬¡æ•°åˆ†å¸ƒ
axes[0, 0].hist(df_final['co_occurrences'], bins=50, edgecolor='black', alpha=0.7)
axes[0, 0].set_xlabel('å…±ç°æ¬¡æ•°')
axes[0, 0].set_ylabel('é¢‘æ•°')
axes[0, 0].set_title('å…±ç°æ¬¡æ•°åˆ†å¸ƒ')
axes[0, 0].set_yscale('log')

# 2. è¾¹æƒé‡åˆ†å¸ƒ
axes[0, 1].hist(df_final['edge_weight'], bins=50, edgecolor='black', alpha=0.7, color='orange')
axes[0, 1].set_xlabel('è¾¹æƒé‡')
axes[0, 1].set_ylabel('é¢‘æ•°')
axes[0, 1].set_title('è¾¹æƒé‡åˆ†å¸ƒ')

# 3. å¹´ä»½è¾¹æ•°å˜åŒ–
year_counts = df_final.groupby('year')['edge_id'].count()
axes[1, 0].plot(year_counts.index, year_counts.values, marker='o')
axes[1, 0].set_xlabel('å¹´ä»½')
axes[1, 0].set_ylabel('è¾¹æ•°')
axes[1, 0].set_title('å„å¹´ä»½è¾¹æ•°å˜åŒ–')
axes[1, 0].grid(True, alpha=0.3)

# 4. å…±ç°æ¬¡æ•°å¹´åº¦å˜åŒ–
year_cooc = df_final.groupby('year')['co_occurrences'].agg(['mean', 'max'])
axes[1, 1].plot(year_cooc.index, year_cooc['mean'], marker='o', label='å¹³å‡å€¼')
axes[1, 1].plot(year_cooc.index, year_cooc['max'], marker='s', label='æœ€å¤§å€¼', alpha=0.7)
axes[1, 1].set_xlabel('å¹´ä»½')
axes[1, 1].set_ylabel('å…±ç°æ¬¡æ•°')
axes[1, 1].set_title('å…±ç°æ¬¡æ•°å¹´åº¦å˜åŒ–')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plot_filename = f"keyword_co_occurrence_stats_{timestamp}.png"
plt.savefig(os.path.join(DATA_DIR, plot_filename), dpi=150)
print(f"ğŸ“ˆ ç»Ÿè®¡å›¾å·²ä¿å­˜: {plot_filename}")

print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
