"""
å¼±é“¾æ¥æ¼”åŒ–é¢„æµ‹æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
å¯¹æ¯”å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¯„ä¼°åœ¨ä¸åŒæ—¶é—´çª—å£çš„é¢„æµ‹æ•ˆæœ
"""

import pandas as pd
import numpy as np
import os
import warnings
from datetime import datetime
import pickle
import json
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix, 
                           classification_report, roc_curve)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
warnings.filterwarnings('ignore')

# é…ç½®
DATA_DIR = 'F:/WLJ/Weak-Link-Effect-Research/data'
MODEL_DIR = os.path.join(DATA_DIR, 'models')
os.makedirs(MODEL_DIR, exist_ok=True)

# è®¾ç½®éšæœºç§å­
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("=" * 80)
print("å¼±é“¾æ¥æ¼”åŒ–é¢„æµ‹æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
print("=" * 80)

# 1. åŠ è½½æ•°æ®
print("\n[1] åŠ è½½ç‰¹å¾æ•°æ®...")

# æŸ¥æ‰¾æœ€æ–°çš„X1X2X3ç‰¹å¾æ–‡ä»¶
feature_files = [f for f in os.listdir(DATA_DIR) if f.startswith('RES04_features_X1X2X3_complete_')]
if not feature_files:
    raise FileNotFoundError("æœªæ‰¾åˆ°X1X2X3ç‰¹å¾æ–‡ä»¶ï¼")

latest_feature_file = sorted(feature_files)[-1]
feature_path = os.path.join(DATA_DIR, latest_feature_file)
print(f"  ä½¿ç”¨ç‰¹å¾æ–‡ä»¶: {latest_feature_file}")

df = pd.read_csv(feature_path)
print(f"  æ ·æœ¬æ€»æ•°: {len(df):,}")
print(f"  æ­£æ ·æœ¬(Y=1): {(df['y']==1).sum():,} ({(df['y']==1).sum()/len(df)*100:.1f}%)")
print(f"  è´Ÿæ ·æœ¬(Y=0): {(df['y']==0).sum():,} ({(df['y']==0).sum()/len(df)*100:.1f}%)")

# 2. ç‰¹å¾é€‰æ‹©
print("\n[2] å‡†å¤‡ç‰¹å¾...")

# å®šä¹‰ç‰¹å¾åˆ—
x1_features = ['link_strength', 'degree_difference', 'betweenness', 'tech_distance']
x2_features = ['common_neighbors', 'adamic_adar', 'resource_allocation', 'preferential_attachment']
x3_features = [
    'innovation_density', 'innovation_avg_clustering', 'innovation_diameter',
    'innovation_degree_centrality_std', 'innovation_modularity',
    'inventor_density', 'inventor_avg_clustering', 'inventor_diameter', 
    'inventor_degree_centrality_std', 'inventor_modularity'
]

all_features = x1_features + x2_features + x3_features
print(f"  ç‰¹å¾æ€»æ•°: {len(all_features)}")
print(f"    - X1ç‰¹å¾: {len(x1_features)}ä¸ª")
print(f"    - X2ç‰¹å¾: {len(x2_features)}ä¸ª")
print(f"    - X3ç‰¹å¾: {len(x3_features)}ä¸ª")

# æ£€æŸ¥ç‰¹å¾å®Œæ•´æ€§
missing_features = [f for f in all_features if f not in df.columns]
if missing_features:
    print(f"  âš ï¸ ç¼ºå¤±ç‰¹å¾: {missing_features}")
    all_features = [f for f in all_features if f in df.columns]

# å‡†å¤‡ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡
X = df[all_features].values
y = df['y'].values

# è·å–æ—¶é—´çª—å£ä¿¡æ¯
df['window_id'] = df['window_t_start'].astype(str) + '-' + df['window_t_end'].astype(str)
unique_windows = df['window_id'].unique()
print(f"\n  æ—¶é—´çª—å£æ•°: {len(unique_windows)}")
for window in sorted(unique_windows):
    window_samples = df[df['window_id'] == window]
    print(f"    {window}: {len(window_samples):,} æ ·æœ¬ "
          f"(Y=1: {(window_samples['y']==1).sum():,})")

# 3. å®šä¹‰æ¨¡å‹
print("\n[3] åˆå§‹åŒ–æ¨¡å‹...")

models = {
    'Logistic Regression': LogisticRegression(
        max_iter=1000, 
        random_state=RANDOM_STATE,
        class_weight='balanced'
    ),
    
    'Decision Tree': DecisionTreeClassifier(
        max_depth=10,
        min_samples_split=20,
        min_samples_leaf=10,
        random_state=RANDOM_STATE,
        class_weight='balanced'
    ),
    
    'Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=RANDOM_STATE,
        class_weight='balanced',
        n_jobs=-1
    ),
    
    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=RANDOM_STATE
    ),
    
    'XGBoost': xgb.XGBClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=6,
        random_state=RANDOM_STATE,
        use_label_encoder=False,
        eval_metric='logloss',
        scale_pos_weight=(y==0).sum()/(y==1).sum()  # å¤„ç†ä¸å¹³è¡¡
    ),
    
    'LightGBM': lgb.LGBMClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=6,
        random_state=RANDOM_STATE,
        class_weight='balanced',
        verbosity=-1
    )
}

print(f"  å‡†å¤‡è®­ç»ƒ {len(models)} ä¸ªæ¨¡å‹")

# 4. æ•´ä½“æ•°æ®é›†åˆ’åˆ†
print("\n[4] æ•°æ®é›†åˆ’åˆ†...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
)

print(f"  è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬")
print(f"  æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬")

# 5. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
print("\n[5] è®­ç»ƒæ¨¡å‹...")

def evaluate_model(y_true, y_pred, y_prob=None):
    """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'auc_roc': roc_auc_score(y_true, y_prob) if y_prob is not None else 0
    }
    return metrics

# å­˜å‚¨ç»“æœ
overall_results = {}
window_results = {}
best_model = None
best_score = 0

# è®­ç»ƒæ¯ä¸ªæ¨¡å‹
for model_name, model in tqdm(models.items(), desc="è®­ç»ƒæ¨¡å‹"):
    print(f"\n  è®­ç»ƒ {model_name}...")
    
    try:
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è·å–æ¦‚ç‡é¢„æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(model, 'predict_proba'):
            y_prob_train = model.predict_proba(X_train)[:, 1]
            y_prob_test = model.predict_proba(X_test)[:, 1]
        else:
            y_prob_train = y_prob_test = None
        
        # è¯„ä¼°æ•´ä½“æ€§èƒ½
        train_metrics = evaluate_model(y_train, y_pred_train, y_prob_train)
        test_metrics = evaluate_model(y_test, y_pred_test, y_prob_test)
        
        # 5æŠ˜äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
        
        overall_results[model_name] = {
            'train': train_metrics,
            'test': test_metrics,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        if test_metrics['f1'] > best_score:
            best_score = test_metrics['f1']
            best_model = (model_name, model)
        
        print(f"    æµ‹è¯•é›† - F1: {test_metrics['f1']:.4f}, AUC: {test_metrics['auc_roc']:.4f}")
        
    except Exception as e:
        print(f"    âŒ è®­ç»ƒå¤±è´¥: {e}")
        overall_results[model_name] = None

# 6. æ—¶é—´çª—å£è¯„ä¼°
print("\n[6] æ—¶é—´çª—å£è¯„ä¼°...")

# å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œæ—¶é—´çª—å£è¯„ä¼°
if best_model:
    model_name, model = best_model
    print(f"  ä½¿ç”¨æœ€ä½³æ¨¡å‹: {model_name}")
    
    for window in sorted(unique_windows):
        # è·å–è¯¥çª—å£çš„æ•°æ®
        window_mask = df['window_id'] == window
        X_window = df[window_mask][all_features].values
        y_window = df[window_mask]['y'].values
        
        if len(y_window) < 50:  # æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
            continue
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        if len(np.unique(y_window)) > 1:  # ç¡®ä¿æœ‰ä¸¤ä¸ªç±»åˆ«
            X_w_train, X_w_test, y_w_train, y_w_test = train_test_split(
                X_window, y_window, test_size=0.3, random_state=RANDOM_STATE, stratify=y_window
            )
            
            # è®­ç»ƒ
            temp_model = models[model_name].__class__(**models[model_name].get_params())
            temp_model.fit(X_w_train, y_w_train)
            
            # é¢„æµ‹
            y_w_pred = temp_model.predict(X_w_test)
            y_w_prob = temp_model.predict_proba(X_w_test)[:, 1] if hasattr(temp_model, 'predict_proba') else None
            
            # è¯„ä¼°
            window_metrics = evaluate_model(y_w_test, y_w_pred, y_w_prob)
            
            if model_name not in window_results:
                window_results[model_name] = {}
            window_results[model_name][window] = window_metrics

# 7. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
print("\n[7] ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")

# åˆ›å»ºç»“æœDataFrame
results_data = []
for model_name, results in overall_results.items():
    if results:
        row = {'Model': model_name}
        row.update({f'Train_{k}': v for k, v in results['train'].items()})
        row.update({f'Test_{k}': v for k, v in results['test'].items()})
        row['CV_Mean_F1'] = results['cv_mean']
        row['CV_Std_F1'] = results['cv_std']
        results_data.append(row)

df_results = pd.DataFrame(results_data)
df_results = df_results.sort_values('Test_f1', ascending=False)

print("\n" + "=" * 80)
print("æ¨¡å‹è¯„ä¼°ç»“æœï¼ˆæ•´ä½“ï¼‰")
print("=" * 80)
print(df_results.to_string(index=False))

# æ—¶é—´çª—å£ç»“æœ
if window_results:
    print("\n" + "=" * 80)
    print(f"æ—¶é—´çª—å£è¯„ä¼°ç»“æœ - {best_model[0]}")
    print("=" * 80)
    
    window_df_data = []
    for window in sorted(unique_windows):
        if best_model[0] in window_results and window in window_results[best_model[0]]:
            metrics = window_results[best_model[0]][window]
            row = {'Window': window}
            row.update(metrics)
            window_df_data.append(row)
    
    if window_df_data:
        df_window_results = pd.DataFrame(window_df_data)
        print(df_window_results.to_string(index=False))
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        print("\næ—¶é—´çª—å£å¹³å‡æ€§èƒ½:")
        for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']:
            if metric in df_window_results.columns:
                mean_val = df_window_results[metric].mean()
                std_val = df_window_results[metric].std()
                print(f"  {metric}: {mean_val:.4f} Â± {std_val:.4f}")

# 8. ä¿å­˜æœ€ä½³æ¨¡å‹
if best_model:
    model_name, model = best_model
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜æ¨¡å‹
    model_filename = f"best_model_{model_name.replace(' ', '_')}_{timestamp}.pkl"
    model_path = os.path.join(MODEL_DIR, model_filename)
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"\nâœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_filename}")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   æµ‹è¯•é›†F1åˆ†æ•°: {overall_results[model_name]['test']['f1']:.4f}")
    
    # ä¿å­˜æ¨¡å‹é…ç½®å’Œæ€§èƒ½
    config = {
        'model_name': model_name,
        'features': all_features,
        'feature_count': len(all_features),
        'timestamp': timestamp,
        'performance': overall_results[model_name],
        'data_info': {
            'total_samples': len(df),
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'positive_ratio': (y==1).sum() / len(y)
        }
    }
    
    config_filename = f"model_config_{timestamp}.json"
    config_path = os.path.join(MODEL_DIR, config_filename)
    
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"   é…ç½®æ–‡ä»¶: {config_filename}")

# 9. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆé’ˆå¯¹æ ‘æ¨¡å‹ï¼‰
if best_model and hasattr(best_model[1], 'feature_importances_'):
    print("\n[8] ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    feature_importance = pd.DataFrame({
        'feature': all_features,
        'importance': best_model[1].feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nTop 10 é‡è¦ç‰¹å¾:")
    print(feature_importance.head(10).to_string(index=False))
    
    # æŒ‰ç‰¹å¾ç»„ç»Ÿè®¡
    x1_importance = feature_importance[feature_importance['feature'].isin(x1_features)]['importance'].sum()
    x2_importance = feature_importance[feature_importance['feature'].isin(x2_features)]['importance'].sum()
    x3_importance = feature_importance[feature_importance['feature'].isin(x3_features)]['importance'].sum()
    
    print(f"\nç‰¹å¾ç»„é‡è¦æ€§:")
    print(f"  X1ç‰¹å¾æ€»é‡è¦æ€§: {x1_importance:.4f}")
    print(f"  X2ç‰¹å¾æ€»é‡è¦æ€§: {x2_importance:.4f}")
    print(f"  X3ç‰¹å¾æ€»é‡è¦æ€§: {x3_importance:.4f}")
    
    # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance.head(15)['feature'], feature_importance.head(15)['importance'])
    plt.xlabel('Importance')
    plt.title(f'Top 15 Feature Importance - {best_model[0]}')
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plot_filename = f"feature_importance_{timestamp}.png"
    plt.savefig(os.path.join(MODEL_DIR, plot_filename), dpi=100, bbox_inches='tight')
    print(f"  ç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜ä¸º: {plot_filename}")
    plt.close()

# 10. ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š
print("\n[9] ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š...")

report_filename = f"model_evaluation_report_{timestamp}.txt"
report_path = os.path.join(MODEL_DIR, report_filename)

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("=" * 80 + "\n")
    f.write("å¼±é“¾æ¥æ¼”åŒ–é¢„æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write("=" * 80 + "\n\n")
    
    # æ•°æ®æ¦‚å†µ
    f.write("ã€æ•°æ®æ¦‚å†µã€‘\n")
    f.write(f"æ•°æ®æ–‡ä»¶: {latest_feature_file}\n")
    f.write(f"æ ·æœ¬æ€»æ•°: {len(df):,}\n")
    f.write(f"æ­£æ ·æœ¬æ•°: {(df['y']==1).sum():,} ({(df['y']==1).sum()/len(df)*100:.1f}%)\n")
    f.write(f"è´Ÿæ ·æœ¬æ•°: {(df['y']==0).sum():,} ({(df['y']==0).sum()/len(df)*100:.1f}%)\n")
    f.write(f"ç‰¹å¾æ•°é‡: {len(all_features)}\n")
    f.write(f"æ—¶é—´çª—å£æ•°: {len(unique_windows)}\n\n")
    
    # ç‰¹å¾åˆ—è¡¨
    f.write("ã€ç‰¹å¾åˆ—è¡¨ã€‘\n")
    f.write(f"X1ç‰¹å¾ ({len(x1_features)}ä¸ª): {', '.join(x1_features)}\n")
    f.write(f"X2ç‰¹å¾ ({len(x2_features)}ä¸ª): {', '.join(x2_features)}\n")
    f.write(f"X3ç‰¹å¾ ({len(x3_features)}ä¸ª): {', '.join(x3_features)}\n\n")
    
    # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    f.write("ã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”ã€‘\n")
    f.write(df_results.to_string(index=False))
    f.write("\n\n")
    
    # æœ€ä½³æ¨¡å‹è¯¦æƒ…
    if best_model:
        f.write("ã€æœ€ä½³æ¨¡å‹ã€‘\n")
        f.write(f"æ¨¡å‹åç§°: {best_model[0]}\n")
        f.write(f"æµ‹è¯•é›†æ€§èƒ½:\n")
        for metric, value in overall_results[best_model[0]]['test'].items():
            f.write(f"  {metric}: {value:.4f}\n")
        f.write(f"äº¤å‰éªŒè¯F1: {overall_results[best_model[0]]['cv_mean']:.4f} Â± {overall_results[best_model[0]]['cv_std']:.4f}\n\n")
        
        # æ—¶é—´çª—å£æ€§èƒ½
        if window_results and best_model[0] in window_results:
            f.write("ã€æ—¶é—´çª—å£æ€§èƒ½ã€‘\n")
            for window in sorted(unique_windows):
                if window in window_results[best_model[0]]:
                    metrics = window_results[best_model[0]][window]
                    f.write(f"\nçª—å£ {window}:\n")
                    for metric, value in metrics.items():
                        f.write(f"  {metric}: {value:.4f}\n")
        
        # ç‰¹å¾é‡è¦æ€§
        if hasattr(best_model[1], 'feature_importances_'):
            f.write("\nã€ç‰¹å¾é‡è¦æ€§Top10ã€‘\n")
            for _, row in feature_importance.head(10).iterrows():
                f.write(f"  {row['feature']}: {row['importance']:.4f}\n")

print(f"  è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")

# 11. æ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Šï¼ˆé’ˆå¯¹æœ€ä½³æ¨¡å‹ï¼‰
if best_model:
    print("\n[10] ç”Ÿæˆæ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Š...")
    
    # é‡æ–°é¢„æµ‹æµ‹è¯•é›†
    y_pred = best_model[1].predict(X_test)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Weakâ†’Weak', 'Weakâ†’Strong'],
                yticklabels=['Weakâ†’Weak', 'Weakâ†’Strong'])
    plt.title(f'Confusion Matrix - {best_model[0]}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    
    # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯
    accuracy = (cm[0,0] + cm[1,1]) / cm.sum()
    plt.text(0.5, -0.1, f'Accuracy: {accuracy:.3f}', 
             ha='center', transform=plt.gca().transAxes)
    
    # ä¿å­˜æ··æ·†çŸ©é˜µå›¾
    cm_filename = f"confusion_matrix_{timestamp}.png"
    plt.savefig(os.path.join(MODEL_DIR, cm_filename), dpi=100, bbox_inches='tight')
    print(f"  æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_filename}")
    plt.close()
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, 
                               target_names=['Weakâ†’Weak', 'Weakâ†’Strong'],
                               digits=4))
    
    # 12. ROCæ›²çº¿
    if hasattr(best_model[1], 'predict_proba'):
        y_prob = best_model[1].predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds = roc_curve(y_test, y_prob)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC curve (AUC = {roc_auc_score(y_test, y_prob):.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {best_model[0]}')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜ROCæ›²çº¿
        roc_filename = f"roc_curve_{timestamp}.png"
        plt.savefig(os.path.join(MODEL_DIR, roc_filename), dpi=100, bbox_inches='tight')
        print(f"  ROCæ›²çº¿å·²ä¿å­˜: {roc_filename}")
        plt.close()

# 13. ç”Ÿæˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
print("\n[11] ç”Ÿæˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾...")

# å‡†å¤‡æ•°æ®
metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']
model_names = []
metric_values = {metric: [] for metric in metrics_to_plot}

for model_name, results in overall_results.items():
    if results:
        model_names.append(model_name)
        for metric in metrics_to_plot:
            metric_values[metric].append(results['test'][metric])

# åˆ›å»ºå¯¹æ¯”å›¾
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx]
    bars = ax.bar(range(len(model_names)), metric_values[metric])
    
    # ä¸ºæœ€é«˜å€¼çš„æŸ±å­ç€è‰²
    max_idx = np.argmax(metric_values[metric])
    bars[max_idx].set_color('green')
    
    ax.set_xticks(range(len(model_names)))
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.set_ylabel(metric.upper())
    ax.set_title(f'{metric.upper()} Comparison')
    ax.set_ylim([0, 1.05])
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(metric_values[metric]):
        ax.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
    
    ax.grid(True, alpha=0.3, axis='y')

# éšè—å¤šä½™çš„å­å›¾
if len(metrics_to_plot) < 6:
    axes[-1].set_visible(False)

plt.suptitle('Model Performance Comparison', fontsize=16, y=1.02)
plt.tight_layout()

# ä¿å­˜å¯¹æ¯”å›¾
comparison_filename = f"model_comparison_{timestamp}.png"
plt.savefig(os.path.join(MODEL_DIR, comparison_filename), dpi=100, bbox_inches='tight')
print(f"  æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜: {comparison_filename}")
plt.close()

# 14. ä¿å­˜æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆå¯é€‰ï¼‰
print("\n[12] ä¿å­˜é¢„æµ‹ç»“æœ...")

predictions_df = pd.DataFrame({
    'y_true': y_test
})

for model_name, model in models.items():
    if model_name in overall_results and overall_results[model_name]:
        try:
            # ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹
            model.fit(X_train, y_train)  # ç¡®ä¿æ¨¡å‹å·²è®­ç»ƒ
            y_pred = model.predict(X_test)
            predictions_df[f'{model_name}_pred'] = y_pred
            
            if hasattr(model, 'predict_proba'):
                y_prob = model.predict_proba(X_test)[:, 1]
                predictions_df[f'{model_name}_prob'] = y_prob
        except:
            pass

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions_filename = f"predictions_{timestamp}.csv"
predictions_df.to_csv(os.path.join(MODEL_DIR, predictions_filename), index=False)
print(f"  é¢„æµ‹ç»“æœå·²ä¿å­˜: {predictions_filename}")

# 15. ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
print("\n" + "=" * 80)
print("æ¨¡å‹è®­ç»ƒå®Œæˆ - æ‘˜è¦")
print("=" * 80)

if best_model:
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
    print(f"   - æµ‹è¯•é›†F1åˆ†æ•°: {overall_results[best_model[0]]['test']['f1']:.4f}")
    print(f"   - æµ‹è¯•é›†AUC-ROC: {overall_results[best_model[0]]['test']['auc_roc']:.4f}")
    print(f"   - æµ‹è¯•é›†å‡†ç¡®ç‡: {overall_results[best_model[0]]['test']['accuracy']:.4f}")
    
    # è®¡ç®—æå‡æ¯”ä¾‹ï¼ˆç›¸å¯¹äºéšæœºçŒœæµ‹ï¼‰
    baseline_f1 = 2 * (y_test.sum()/len(y_test)) * 0.5 / ((y_test.sum()/len(y_test)) + 0.5)
    improvement = (overall_results[best_model[0]]['test']['f1'] - baseline_f1) / baseline_f1 * 100
    print(f"   - ç›¸å¯¹åŸºçº¿æå‡: {improvement:.1f}%")

print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®: {MODEL_DIR}")
print(f"   - æ¨¡å‹æ–‡ä»¶: {model_filename}")
print(f"   - é…ç½®æ–‡ä»¶: {config_filename}")
print(f"   - è¯„ä¼°æŠ¥å‘Š: {report_filename}")
print(f"   - æ··æ·†çŸ©é˜µ: {cm_filename}")
if 'roc_filename' in locals():
    print(f"   - ROCæ›²çº¿: {roc_filename}")
print(f"   - æ¨¡å‹å¯¹æ¯”å›¾: {comparison_filename}")
print(f"   - é¢„æµ‹ç»“æœ: {predictions_filename}")

print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
print("=" * 80)

# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
print("\nã€æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹ã€‘")
print("```python")
print("import pickle")
print("import numpy as np")
print("")
print(f"# åŠ è½½æ¨¡å‹")
print(f"with open('{model_path}', 'rb') as f:")
print("    model = pickle.load(f)")
print("")
print("# å‡†å¤‡ç‰¹å¾ï¼ˆ18ç»´ï¼‰")
print(f"features = np.array([[...]])  # {len(all_features)}ä¸ªç‰¹å¾")
print("")
print("# é¢„æµ‹")
print("prediction = model.predict(features)")
print("probability = model.predict_proba(features)[:, 1]")
print("")
print("print(f'é¢„æµ‹ç»“æœ: {prediction[0]}')")
print("print(f'æ¼”åŒ–ä¸ºå¼ºé“¾æ¥æ¦‚ç‡: {probability[0]:.3f}')")
print("```")
