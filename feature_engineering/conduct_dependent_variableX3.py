"""
X3ç‰¹å¾è®¡ç®—è„šæœ¬ - å…³è”ç‰¹å¾ï¼ˆåˆ›æ–°ç‚¹è¯­ä¹‰ç½‘ç»œ + å‘æ˜äººåˆä½œç½‘ç»œï¼‰
ä½¿ç”¨å‘é‡åŒ–æ“ä½œä¼˜åŒ–æ€§èƒ½
"""

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime
import os
import warnings
from sklearn.preprocessing import MinMaxScaler
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import community as community_louvain
import ast
import time
from scipy import sparse
from collections import defaultdict
warnings.filterwarnings('ignore')

# é…ç½®
DATA_DIR = 'F:/WLJ/Weak-Link-Effect-Research/data'

# è¾“å…¥æ–‡ä»¶
X1X2_FEATURES_FILE = os.path.join(DATA_DIR, "RES03_features_X1X2_complete_20250914_003935.csv")  # éœ€è¦æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶å
KEYWORDS_FILE = os.path.join(DATA_DIR, "Keywords_Processed_20250903_211104.csv")
INNOVATIONS_FILE = os.path.join(DATA_DIR, "Innovations_Extracted_20250901_172340.csv")
INVENTORS_FILE = os.path.join(DATA_DIR, "TYT_DATA_PERSON_INFO_INVENTOR_ONLY.csv")
LINK_STRENGTH_FILE = os.path.join(DATA_DIR, "Keyword_LinkStrength_ByWindow_20250903_212630.csv")

print("=" * 80)
print("X3ç‰¹å¾è®¡ç®— - å…³è”ç‰¹å¾ï¼ˆåˆ›æ–°ç‚¹è¯­ä¹‰ç½‘ç»œ + å‘æ˜äººåˆä½œç½‘ç»œï¼‰")
print("=" * 80)

# 1. è¯»å–æ•°æ®
print("\n[1] åŠ è½½æ•°æ®...")
df_x1x2 = pd.read_csv(X1X2_FEATURES_FILE)
df_keywords = pd.read_csv(KEYWORDS_FILE)
df_innovations = pd.read_csv(INNOVATIONS_FILE)
df_inventors = pd.read_csv(INVENTORS_FILE)
df_links = pd.read_csv(LINK_STRENGTH_FILE)

print(f"  X1X2ç‰¹å¾æ ·æœ¬æ•°: {len(df_x1x2):,}")
print(f"  å…³é”®è¯è®°å½•æ•°: {len(df_keywords):,}")
print(f"  åˆ›æ–°ç‚¹è®°å½•æ•°: {len(df_innovations):,}")
print(f"  å‘æ˜äººè®°å½•æ•°: {len(df_inventors):,}")

# 2. åŠ è½½è¯­ä¹‰æ¨¡å‹
print("\n[2] åŠ è½½è¯­ä¹‰åµŒå…¥æ¨¡å‹...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("  æ¨¡å‹åŠ è½½å®Œæˆ: all-MiniLM-L6-v2")

# 3. æ•°æ®é¢„å¤„ç†
print("\n[3] æ•°æ®é¢„å¤„ç†...")

# è§£æå­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨
def safe_eval_list(x):
    """å®‰å…¨åœ°å°†å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨è½¬æ¢ä¸ºPythonåˆ—è¡¨"""
    if pd.isna(x):
        return []
    if isinstance(x, list):
        return x
    try:
        return ast.literal_eval(x)
    except:
        return []

# å¤„ç†å…³é”®è¯
df_keywords['keywords'] = df_keywords['keywords'].apply(safe_eval_list)
# åˆ›å»ºå…³é”®è¯åˆ°ä¸“åˆ©çš„æ˜ å°„ï¼ˆå‘é‡åŒ–ï¼‰
keyword_to_patents = defaultdict(set)
for idx, row in df_keywords.iterrows():
    for kw in row['keywords']:
        keyword_to_patents[kw].add(row['appln_id'])

# å¤„ç†åˆ›æ–°ç‚¹
df_innovations['innovations'] = df_innovations['innovations'].apply(safe_eval_list)
# åˆ›å»ºä¸“åˆ©åˆ°åˆ›æ–°ç‚¹çš„æ˜ å°„
patent_to_innovations = dict(zip(df_innovations['appln_id'], df_innovations['innovations']))

# åˆ›å»ºä¸“åˆ©åˆ°å‘æ˜äººçš„æ˜ å°„ï¼ˆå‘é‡åŒ–ï¼‰
patent_to_inventors = df_inventors.groupby('appln_id')['person_id'].apply(set).to_dict()

print(f"  å…³é”®è¯æ•°é‡: {len(keyword_to_patents):,}")
print(f"  åŒ…å«åˆ›æ–°ç‚¹çš„ä¸“åˆ©æ•°: {len(patent_to_innovations):,}")
print(f"  åŒ…å«å‘æ˜äººçš„ä¸“åˆ©æ•°: {len(patent_to_inventors):,}")

# 4. å®šä¹‰ç½‘ç»œç‰¹å¾è®¡ç®—å‡½æ•°
def calculate_network_features(G):
    """è®¡ç®—ç½‘ç»œçš„5ä¸ªæ ¸å¿ƒç‰¹å¾"""
    features = {}
    
    if G.number_of_nodes() == 0:
        return {
            'density': 0, 
            'avg_clustering': 0, 
            'diameter': 0, 
            'degree_centrality_std': 0, 
            'modularity': 0
        }
    
    # 1. ç½‘ç»œå¯†åº¦
    features['density'] = nx.density(G) if G.number_of_nodes() > 1 else 0
    
    # 2. å¹³å‡èšç±»ç³»æ•°
    features['avg_clustering'] = nx.average_clustering(G) if G.number_of_nodes() > 0 else 0
    
    # 3. ç½‘ç»œç›´å¾„ï¼ˆä½¿ç”¨æœ€å¤§è¿é€šåˆ†é‡ï¼‰
    if G.number_of_nodes() > 1:
        components = list(nx.connected_components(G))
        if components:
            largest_cc = G.subgraph(max(components, key=len))
            if largest_cc.number_of_nodes() > 1:
                features['diameter'] = nx.diameter(largest_cc)
            else:
                features['diameter'] = 0
        else:
            features['diameter'] = 0
    else:
        features['diameter'] = 0
    
    # 4. åº¦ä¸­å¿ƒæ€§æ ‡å‡†å·®
    if G.number_of_nodes() > 1:
        degree_centrality = nx.degree_centrality(G)
        features['degree_centrality_std'] = np.std(list(degree_centrality.values()))
    else:
        features['degree_centrality_std'] = 0
    
    # 5. æ¨¡å—åº¦
    if G.number_of_edges() > 0:
        try:
            partition = community_louvain.best_partition(G)
            features['modularity'] = community_louvain.modularity(partition, G)
        except:
            features['modularity'] = 0
    else:
        features['modularity'] = 0
    
    return features

# 5. æ‰¹é‡è®¡ç®—åˆ›æ–°ç‚¹embeddingï¼ˆå‘é‡åŒ–ï¼‰
print("\n[4] è®¡ç®—åˆ›æ–°ç‚¹åµŒå…¥å‘é‡...")

# æ”¶é›†æ‰€æœ‰åˆ›æ–°ç‚¹æ–‡æœ¬
all_innovations_text = []
innovation_to_idx = {}
idx_counter = 0

for appln_id, innovations in patent_to_innovations.items():
    for innovation in innovations:
        if innovation not in innovation_to_idx:
            innovation_to_idx[innovation] = idx_counter
            all_innovations_text.append(innovation)
            idx_counter += 1

print(f"  ç‹¬ç‰¹åˆ›æ–°ç‚¹æ•°é‡: {len(all_innovations_text):,}")

# æ‰¹é‡è®¡ç®—embeddingï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
if all_innovations_text:
    print("  æ‰¹é‡è®¡ç®—embedding...")
    batch_size = 256
    all_embeddings = []
    
    for i in range(0, len(all_innovations_text), batch_size):
        batch = all_innovations_text[i:i+batch_size]
        batch_embeddings = model.encode(batch, show_progress_bar=False)
        all_embeddings.append(batch_embeddings)
    
    innovation_embeddings = np.vstack(all_embeddings)
    print(f"  EmbeddingçŸ©é˜µå½¢çŠ¶: {innovation_embeddings.shape}")
else:
    innovation_embeddings = np.array([])

# 6. åˆå§‹åŒ–X3ç‰¹å¾æ•°ç»„
n_samples = len(df_x1x2)
print(f"\n[5] åˆå§‹åŒ–X3ç‰¹å¾æ•°ç»„ (æ ·æœ¬æ•°: {n_samples:,})...")

# åˆ›æ–°ç‚¹ç½‘ç»œç‰¹å¾
innovation_density = np.zeros(n_samples)
innovation_avg_clustering = np.zeros(n_samples)
innovation_diameter = np.zeros(n_samples)
innovation_degree_centrality_std = np.zeros(n_samples)
innovation_modularity = np.zeros(n_samples)

# å‘æ˜äººç½‘ç»œç‰¹å¾
inventor_density = np.zeros(n_samples)
inventor_avg_clustering = np.zeros(n_samples)
inventor_diameter = np.zeros(n_samples)
inventor_degree_centrality_std = np.zeros(n_samples)
inventor_modularity = np.zeros(n_samples)

# 7. æ‰¹é‡å¤„ç†æ¯ä¸ªæ ·æœ¬ï¼ˆè¾¹ï¼‰
print("\n[6] æ‰¹é‡è®¡ç®—X3ç‰¹å¾...")

# åˆ›å»ºæ—¶é—´çª—å£æ ‡è¯†
df_x1x2['window_id'] = df_x1x2['window_t_start'].astype(str) + '-' + df_x1x2['window_t_end'].astype(str)

# æŒ‰æ—¶é—´çª—å£åˆ†ç»„å¤„ç†
unique_windows = df_x1x2['window_id'].unique()
print(f"  å¤„ç† {len(unique_windows)} ä¸ªæ—¶é—´çª—å£...")

for window_idx, window_id in enumerate(unique_windows, 1):
    print(f"\n  çª—å£ {window_idx}/{len(unique_windows)}: {window_id}")
    window_start_time = time.time()
    
    # è·å–è¯¥çª—å£çš„æ ·æœ¬
    window_mask = df_x1x2['window_id'] == window_id
    window_samples = df_x1x2[window_mask]
    window_indices = window_samples.index.values
    
    print(f"    æ ·æœ¬æ•°: {len(window_samples):,}")
    
    # æ‰¹é‡å¤„ç†è¯¥çª—å£çš„æ‰€æœ‰æ ·æœ¬
    for idx in window_indices:
        node_u = df_x1x2.loc[idx, 'node_u']
        node_v = df_x1x2.loc[idx, 'node_v']
        
        # è·å–è¾¹å¯¹åº”çš„ä¸“åˆ©é›†åˆï¼ˆä¸¤ä¸ªå…³é”®è¯çš„ä¸“åˆ©äº¤é›†ï¼‰
        patents_u = keyword_to_patents.get(node_u, set())
        patents_v = keyword_to_patents.get(node_v, set())
        edge_patents = patents_u & patents_v
        
        if len(edge_patents) == 0:
            continue
        
        # === åˆ›æ–°ç‚¹è¯­ä¹‰ç½‘ç»œç‰¹å¾ ===
        edge_innovations = []
        edge_innovation_indices = []
        
        for patent_id in edge_patents:
            if patent_id in patent_to_innovations:
                for innovation in patent_to_innovations[patent_id]:
                    if innovation in innovation_to_idx:
                        edge_innovations.append(innovation)
                        edge_innovation_indices.append(innovation_to_idx[innovation])
        
        if len(edge_innovations) > 1:
            # æå–ç›¸å…³çš„embeddingï¼ˆå‘é‡åŒ–ï¼‰
            edge_embeddings = innovation_embeddings[edge_innovation_indices]
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆå‘é‡åŒ–ï¼‰
            similarity_matrix = cosine_similarity(edge_embeddings)
            
            # æ„å»ºåˆ›æ–°ç‚¹ç›¸ä¼¼ç½‘ç»œï¼ˆè®¾ç½®é˜ˆå€¼ï¼‰
            threshold = 0.5
            G_innovation = nx.Graph()
            G_innovation.add_nodes_from(range(len(edge_innovations)))
            
            # å‘é‡åŒ–æ·»åŠ è¾¹
            edges_to_add = []
            for i in range(len(edge_innovations)):
                for j in range(i+1, len(edge_innovations)):
                    if similarity_matrix[i, j] > threshold:
                        edges_to_add.append((i, j, {'weight': similarity_matrix[i, j]}))
            
            G_innovation.add_edges_from(edges_to_add)
            
            # è®¡ç®—ç½‘ç»œç‰¹å¾
            innovation_features = calculate_network_features(G_innovation)
            
            # èµ‹å€¼ç‰¹å¾
            innovation_density[idx] = innovation_features['density']
            innovation_avg_clustering[idx] = innovation_features['avg_clustering']
            innovation_diameter[idx] = innovation_features['diameter']
            innovation_degree_centrality_std[idx] = innovation_features['degree_centrality_std']
            innovation_modularity[idx] = innovation_features['modularity']
        
        # === å‘æ˜äººåˆä½œç½‘ç»œç‰¹å¾ ===
        edge_inventors = set()
        inventor_collaborations = defaultdict(set)
        
        for patent_id in edge_patents:
            if patent_id in patent_to_inventors:
                patent_inventors = patent_to_inventors[patent_id]
                edge_inventors.update(patent_inventors)
                
                # è®°å½•åˆä½œå…³ç³»ï¼ˆåŒä¸€ä¸“åˆ©çš„å‘æ˜äººä¹‹é—´å­˜åœ¨åˆä½œï¼‰
                for inv1 in patent_inventors:
                    for inv2 in patent_inventors:
                        if inv1 != inv2:
                            inventor_collaborations[inv1].add(inv2)
        
        if len(edge_inventors) > 1:
            # æ„å»ºå‘æ˜äººç½‘ç»œ
            G_inventor = nx.Graph()
            G_inventor.add_nodes_from(edge_inventors)
            
            # å‘é‡åŒ–æ·»åŠ è¾¹
            edges_to_add = []
            for inv1, collaborators in inventor_collaborations.items():
                for inv2 in collaborators:
                    if inv1 < inv2:  # é¿å…é‡å¤æ·»åŠ è¾¹
                        edges_to_add.append((inv1, inv2))
            
            G_inventor.add_edges_from(edges_to_add)
            
            # è®¡ç®—ç½‘ç»œç‰¹å¾
            inventor_features = calculate_network_features(G_inventor)
            
            # èµ‹å€¼ç‰¹å¾
            inventor_density[idx] = inventor_features['density']
            inventor_avg_clustering[idx] = inventor_features['avg_clustering']
            inventor_diameter[idx] = inventor_features['diameter']
            inventor_degree_centrality_std[idx] = inventor_features['degree_centrality_std']
            inventor_modularity[idx] = inventor_features['modularity']
    
    elapsed = time.time() - window_start_time
    print(f"    è€—æ—¶: {elapsed:.2f}ç§’")

# 8. æ·»åŠ X3ç‰¹å¾åˆ°DataFrame
print("\n[7] æ•´åˆX3ç‰¹å¾...")

# åˆ›æ–°ç‚¹ç½‘ç»œç‰¹å¾
df_x1x2['innovation_density'] = innovation_density
df_x1x2['innovation_avg_clustering'] = innovation_avg_clustering
df_x1x2['innovation_diameter'] = innovation_diameter
df_x1x2['innovation_degree_centrality_std'] = innovation_degree_centrality_std
df_x1x2['innovation_modularity'] = innovation_modularity

# å‘æ˜äººç½‘ç»œç‰¹å¾
df_x1x2['inventor_density'] = inventor_density
df_x1x2['inventor_avg_clustering'] = inventor_avg_clustering
df_x1x2['inventor_diameter'] = inventor_diameter
df_x1x2['inventor_degree_centrality_std'] = inventor_degree_centrality_std
df_x1x2['inventor_modularity'] = inventor_modularity

# 9. X3ç‰¹å¾æ ‡å‡†åŒ–
print("\n[8] X3ç‰¹å¾æ ‡å‡†åŒ–...")

x3_feature_cols = [
    'innovation_density', 'innovation_avg_clustering', 'innovation_diameter',
    'innovation_degree_centrality_std', 'innovation_modularity',
    'inventor_density', 'inventor_avg_clustering', 'inventor_diameter',
    'inventor_degree_centrality_std', 'inventor_modularity'
]

# ä¿å­˜åŸå§‹å€¼
for col in x3_feature_cols:
    df_x1x2[f'{col}_raw'] = df_x1x2[col].values

# æ ‡å‡†åŒ–åˆ°[0,1]
scaler = MinMaxScaler()
for col in x3_feature_cols:
    col_values = df_x1x2[col].values.reshape(-1, 1)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ–¹å·®
    if np.std(col_values) > 1e-10:
        df_x1x2[col] = scaler.fit_transform(col_values).flatten()
    else:
        print(f"  è­¦å‘Š: {col} æ–¹å·®è¿‡å°ï¼Œä¿æŒåŸå€¼")
        if np.mean(col_values) > 0:
            df_x1x2[col] = 1.0
        else:
            df_x1x2[col] = 0.0

print("  X3ç‰¹å¾å·²æ ‡å‡†åŒ–åˆ° [0, 1] åŒºé—´")

# 10. æ•´ç†æœ€ç»ˆæ•°æ®ï¼ˆX1 + X2 + X3ï¼‰
print("\n[9] æ•´ç†æœ€ç»ˆæ•°æ®ï¼ˆX1 + X2 + X3ï¼‰...")

# ç¡®å®šæ‰€æœ‰ç‰¹å¾åˆ—
x1_feature_cols = ['link_strength', 'degree_difference', 'betweenness', 'tech_distance']
x2_feature_cols = ['common_neighbors', 'adamic_adar', 'resource_allocation', 'preferential_attachment']
all_feature_cols = x1_feature_cols + x2_feature_cols + x3_feature_cols

# ç§»é™¤ä¸´æ—¶åˆ—
if 'window_id' in df_x1x2.columns:
    df_x1x2.drop('window_id', axis=1, inplace=True)

# 11. ä¿å­˜ç»“æœ
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"RES04_features_X1X2X3_complete_{timestamp}.csv"
output_path = os.path.join(DATA_DIR, output_filename)

df_x1x2.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n" + "=" * 80)
print(f"âœ… X1+X2+X3ç‰¹å¾è®¡ç®—å®Œæˆï¼")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
print(f"   è·¯å¾„: {output_path}")
print(f"   æ ·æœ¬æ•°: {len(df_x1x2):,}")
print(f"   ç‰¹å¾æ€»æ•°: {len(all_feature_cols)} ä¸ª")
print(f"     - X1ç‰¹å¾: {len(x1_feature_cols)} ä¸ª")
print(f"     - X2ç‰¹å¾: {len(x2_feature_cols)} ä¸ª")
print(f"     - X3ç‰¹å¾: {len(x3_feature_cols)} ä¸ª")
print("=" * 80)

# 12. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
print("\n[10] ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

# X3ç‰¹å¾ç»Ÿè®¡
print("\nX3ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:")
print(df_x1x2[x3_feature_cols].describe())

# æŒ‰Yå€¼åˆ†ç»„ç»Ÿè®¡
print("\næŒ‰Yå€¼åˆ†ç»„çš„X3ç‰¹å¾å‡å€¼ï¼ˆæ ‡å‡†åŒ–åï¼‰:")
grouped_x3 = df_x1x2.groupby('y')[x3_feature_cols].mean()
print(grouped_x3)

# ç‰¹å¾ç›¸å…³æ€§
print("\nX3ç‰¹å¾å†…éƒ¨ç›¸å…³æ€§:")
corr_matrix_x3 = df_x1x2[x3_feature_cols].corr()
print(corr_matrix_x3)

# 13. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ–‡ä»¶
report_filename = f"RES04_features_X1X2X3_report_{timestamp}.txt"
report_path = os.path.join(DATA_DIR, report_filename)

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("X1+X2+X3å®Œæ•´ç‰¹å¾è®¡ç®—æŠ¥å‘Š\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    f.write("æ•°æ®æ¦‚å†µ:\n")
    f.write(f"  æ ·æœ¬æ€»æ•°: {len(df_x1x2):,}\n")
    f.write(f"  æ­£æ ·æœ¬æ•°: {(df_x1x2['y']==1).sum():,}\n")
    f.write(f"  è´Ÿæ ·æœ¬æ•°: {(df_x1x2['y']==0).sum():,}\n")
    f.write(f"  ç‰¹å¾æ€»æ•°: {len(all_feature_cols)}\n")
    f.write(f"    - X1ç‰¹å¾: {len(x1_feature_cols)}ä¸ª\n")
    f.write(f"    - X2ç‰¹å¾: {len(x2_feature_cols)}ä¸ª\n")
    f.write(f"    - X3ç‰¹å¾: {len(x3_feature_cols)}ä¸ª\n\n")
    
    f.write("X3ç‰¹å¾è¯´æ˜:\n")
    f.write("åˆ›æ–°ç‚¹è¯­ä¹‰ç½‘ç»œç‰¹å¾:\n")
    f.write("  - innovation_density: åˆ›æ–°ç‚¹ç½‘ç»œå¯†åº¦\n")
    f.write("  - innovation_avg_clustering: åˆ›æ–°ç‚¹ç½‘ç»œå¹³å‡èšç±»ç³»æ•°\n")
    f.write("  - innovation_diameter: åˆ›æ–°ç‚¹ç½‘ç»œç›´å¾„\n")
    f.write("  - innovation_degree_centrality_std: åˆ›æ–°ç‚¹ç½‘ç»œåº¦ä¸­å¿ƒæ€§æ ‡å‡†å·®\n")
    f.write("  - innovation_modularity: åˆ›æ–°ç‚¹ç½‘ç»œæ¨¡å—åº¦\n\n")
    
    f.write("å‘æ˜äººåˆä½œç½‘ç»œç‰¹å¾:\n")
    f.write("  - inventor_density: å‘æ˜äººç½‘ç»œå¯†åº¦\n")
    f.write("  - inventor_avg_clustering: å‘æ˜äººç½‘ç»œå¹³å‡èšç±»ç³»æ•°\n")
    f.write("  - inventor_diameter: å‘æ˜äººç½‘ç»œç›´å¾„\n")
    f.write("  - inventor_degree_centrality_std: å‘æ˜äººç½‘ç»œåº¦ä¸­å¿ƒæ€§æ ‡å‡†å·®\n")
    f.write("  - inventor_modularity: å‘æ˜äººç½‘ç»œæ¨¡å—åº¦\n\n")
    
    f.write("X3ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:\n")
    f.write(df_x1x2[x3_feature_cols].describe().to_string())
    f.write("\n\n")
    
    f.write("æŒ‰Yå€¼åˆ†ç»„çš„X3ç‰¹å¾å‡å€¼å¯¹æ¯”:\n")
    f.write(grouped_x3.to_string())
    f.write("\n\n")
    
    # Y=0 vs Y=1 çš„å·®å¼‚åˆ†æ
    f.write("Y=0 vs Y=1 X3ç‰¹å¾å·®å¼‚åˆ†æ:\n")
    for col in x3_feature_cols:
        mean_0 = df_x1x2[df_x1x2['y']==0][col].mean()
        mean_1 = df_x1x2[df_x1x2['y']==1][col].mean()
        if mean_0 != 0:
            ratio = mean_1 / mean_0
            diff_pct = (mean_1 - mean_0) / mean_0 * 100
            f.write(f"  {col}: Y=1/Y=0 = {ratio:.3f} (å·®å¼‚: {diff_pct:+.1f}%)\n")
        else:
            f.write(f"  {col}: Y=0å‡å€¼ä¸º0\n")
    
    f.write("\n")
    
    # ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆåŸºäºY=1å’ŒY=0çš„å·®å¼‚ï¼‰
    f.write("X3ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆåŸºäºYå€¼å·®å¼‚ï¼‰:\n")
    feature_importance = []
    for col in x3_feature_cols:
        mean_0 = df_x1x2[df_x1x2['y']==0][col].mean()
        mean_1 = df_x1x2[df_x1x2['y']==1][col].mean()
        diff = abs(mean_1 - mean_0)
        feature_importance.append((col, diff))
    
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    for i, (col, diff) in enumerate(feature_importance, 1):
        f.write(f"  {i}. {col}: å·®å¼‚å€¼ = {diff:.4f}\n")
    
    f.write("\n")
    
    if 'patent_level' in df_x1x2.columns:
        f.write("æŒ‰ä¸“åˆ©ç­‰çº§çš„X3ç‰¹å¾å‡å€¼:\n")
        patent_x3_stats = df_x1x2.groupby('patent_level')[x3_feature_cols].mean()
        f.write(patent_x3_stats.to_string())
        f.write("\n\n")
    
    f.write("X3ç‰¹å¾å†…éƒ¨ç›¸å…³æ€§çŸ©é˜µ:\n")
    f.write(corr_matrix_x3.to_string())
    f.write("\n\n")
    
    # é«˜ç›¸å…³ç‰¹å¾å¯¹
    f.write("X3é«˜ç›¸å…³ç‰¹å¾å¯¹ï¼ˆ|r| > 0.7ï¼‰:\n")
    high_corr_pairs = []
    for i in range(len(x3_feature_cols)):
        for j in range(i+1, len(x3_feature_cols)):
            corr_val = corr_matrix_x3.iloc[i, j]
            if abs(corr_val) > 0.7:
                high_corr_pairs.append((x3_feature_cols[i], x3_feature_cols[j], corr_val))
    
    if high_corr_pairs:
        for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):
            f.write(f"  {feat1} <-> {feat2}: {corr:.3f}\n")
    else:
        f.write("  æ— é«˜ç›¸å…³ç‰¹å¾å¯¹\n")
    
    f.write("\nè®¡ç®—æ–¹æ³•è¯´æ˜:\n")
    f.write("1. å¯¹æ¯æ¡è¾¹ï¼ˆå…³é”®è¯å¯¹ï¼‰ï¼Œæ‰¾åˆ°åŒ…å«è¿™ä¸¤ä¸ªå…³é”®è¯çš„ä¸“åˆ©é›†åˆ\n")
    f.write("2. åŸºäºä¸“åˆ©é›†åˆçš„åˆ›æ–°ç‚¹ï¼Œæ„å»ºè¯­ä¹‰ç›¸ä¼¼ç½‘ç»œï¼ˆç›¸ä¼¼åº¦>0.5ï¼‰\n")
    f.write("3. åŸºäºä¸“åˆ©é›†åˆçš„å‘æ˜äººï¼Œæ„å»ºåˆä½œç½‘ç»œ\n")
    f.write("4. è®¡ç®—ä¸¤ä¸ªç½‘ç»œçš„æ‹“æ‰‘ç‰¹å¾æŒ‡æ ‡\n")
    f.write("5. æ ‡å‡†åŒ–åˆ°[0,1]åŒºé—´\n\n")
    
    f.write("ä¼˜åŒ–æŠ€æœ¯:\n")
    f.write("  1. æ‰¹é‡è®¡ç®—embeddingï¼ˆå‘é‡åŒ–ï¼‰\n")
    f.write("  2. ä½¿ç”¨cosine_similarityæ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦\n")
    f.write("  3. å‘é‡åŒ–çš„è¾¹æ·»åŠ æ“ä½œ\n")
    f.write("  4. é¢„è®¡ç®—å’Œç¼“å­˜æ˜ å°„å…³ç³»\n")
    f.write("  5. æŒ‰æ—¶é—´çª—å£æ‰¹å¤„ç†\n")

print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}")

# 14. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
print("\n[11] æ•°æ®å®Œæ•´æ€§æ£€æŸ¥...")

# æ£€æŸ¥ç¼ºå¤±å€¼
missing_features = df_x1x2[all_feature_cols].isna().sum()
if missing_features.sum() > 0:
    print("  è­¦å‘Šï¼šå­˜åœ¨ç¼ºå¤±å€¼")
    print(missing_features[missing_features > 0])
else:
    print("  âœ“ æ‰€æœ‰ç‰¹å¾å€¼å®Œæ•´")

# æ£€æŸ¥å¼‚å¸¸å€¼
for col in x3_feature_cols:
    outliers = ((df_x1x2[col] < 0) | (df_x1x2[col] > 1)).sum()
    if outliers > 0:
        print(f"  è­¦å‘Š: {col} æœ‰ {outliers} ä¸ªå€¼è¶…å‡º[0,1]èŒƒå›´")

# ç‰¹å¾åˆ†å¸ƒæ£€æŸ¥
print("\nX3ç‰¹å¾å€¼åˆ†å¸ƒæ£€æŸ¥:")
for col in x3_feature_cols:
    non_zero = (df_x1x2[col] > 0).sum()
    zero_pct = (1 - non_zero/len(df_x1x2)) * 100
    print(f"  {col}: {zero_pct:.1f}%ä¸º0å€¼, {non_zero:,}ä¸ªéé›¶å€¼")

print("\nâœ… X1+X2+X3å®Œæ•´ç‰¹å¾é›†è®¡ç®—å®Œæˆï¼")
print(f"   X3ç‰¹å¾äº®ç‚¹: ")
print(f"   - åˆ›æ–°ç‚¹è¯­ä¹‰ç½‘ç»œ: åŸºäºembeddingç›¸ä¼¼åº¦æ„å»º")
print(f"   - å‘æ˜äººåˆä½œç½‘ç»œ: åŸºäºä¸“åˆ©åˆä½œå…³ç³»æ„å»º")
print(f"   - 10ä¸ªç½‘ç»œæ‹“æ‰‘ç‰¹å¾: å¯†åº¦ã€èšç±»ã€ç›´å¾„ã€ä¸­å¿ƒæ€§ã€æ¨¡å—åº¦")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
