"""
æ‰©å±•ç‰¹å¾X2è®¡ç®—è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŸºäºX1ç»“æœè®¡ç®—é¢å¤–çš„ç½‘ç»œç‰¹å¾ï¼Œå¹¶æ‹¼æ¥ä¿å­˜
ä½¿ç”¨å‘é‡åŒ–æ“ä½œé¿å…å¾ªç¯ï¼Œæé«˜è®¡ç®—é€Ÿåº¦
"""

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime
import os
import warnings
from sklearn.preprocessing import MinMaxScaler
import time
from scipy import sparse
warnings.filterwarnings('ignore')

# é…ç½®
DATA_DIR = 'F:/WLJ/Weak-Link-Effect-Research/data'

# è¾“å…¥æ–‡ä»¶
X1_FEATURES_FILE = os.path.join(DATA_DIR, "RES02_features_X1_optimized_20250914_001705.csv")
LINK_STRENGTH_FILE = os.path.join(DATA_DIR, "Keyword_LinkStrength_ByWindow_20250903_212630.csv")

print("=" * 80)
print("æ‰©å±•ç‰¹å¾X2è®¡ç®—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
print("=" * 80)

# 1. è¯»å–æ•°æ®
print("\n[1] è¯»å–æ•°æ®...")
df_x1 = pd.read_csv(X1_FEATURES_FILE)
df_links_all = pd.read_csv(LINK_STRENGTH_FILE)

print(f"  X1ç‰¹å¾æ ·æœ¬æ•°: {len(df_x1):,}")
print(f"  å…¨éƒ¨é“¾æ¥æ•°: {len(df_links_all):,}")

# 2. å‡†å¤‡æ•°æ®
print("\n[2] å‡†å¤‡æ•°æ®...")

# åˆ›å»ºæ—¶é—´çª—å£æ ‡è¯†
df_x1['window_id'] = df_x1['window_t_start'].astype(str) + '-' + df_x1['window_t_end'].astype(str)
df_links_all['window_id'] = df_links_all['window_start'].astype(str) + '-' + df_links_all['window_end'].astype(str)

# 3. åˆå§‹åŒ–X2ç‰¹å¾æ•°ç»„
n_samples = len(df_x1)
common_neighbors = np.zeros(n_samples)
adamic_adar = np.zeros(n_samples)
resource_allocation = np.zeros(n_samples)
preferential_attachment = np.zeros(n_samples)

# 4. è·å–æ—¶é—´çª—å£
unique_windows = df_x1[['window_t_start', 'window_t_end', 'window_id']].drop_duplicates()
print(f"\n[3] å¤„ç† {len(unique_windows)} ä¸ªæ—¶é—´çª—å£...")

# 5. å®šä¹‰å‘é‡åŒ–çš„ç‰¹å¾è®¡ç®—å‡½æ•°
def compute_x2_features_vectorized(window_data):
    """å‘é‡åŒ–è®¡ç®—X2ç‰¹å¾"""
    w_start, w_end, window_id = window_data
    
    # è·å–è¯¥çª—å£çš„æ ·æœ¬
    window_mask = df_x1['window_id'] == window_id
    window_samples = df_x1[window_mask]
    window_indices = window_samples.index.values
    
    if len(window_indices) == 0:
        return None
    
    print(f"    æ ·æœ¬æ•°: {len(window_samples):,}")
    
    # è·å–è¯¥çª—å£çš„é“¾æ¥
    window_links = df_links_all[df_links_all['window_id'] == window_id]
    
    if len(window_links) == 0:
        print(f"    è­¦å‘Šï¼šçª—å£ {window_id} æ²¡æœ‰é“¾æ¥æ•°æ®")
        return None
    
    # æ„å»ºç½‘ç»œ
    G = nx.from_pandas_edgelist(
        window_links,
        source='node_u',
        target='node_v',
        edge_attr='link_strength',
        create_using=nx.Graph()
    )
    
    # ç¡®ä¿æ ·æœ¬èŠ‚ç‚¹éƒ½åœ¨ç½‘ç»œä¸­
    sample_nodes = set(window_samples['node_u']) | set(window_samples['node_v'])
    for node in sample_nodes - set(G.nodes()):
        G.add_node(node)
    
    print(f"    ç½‘ç»œ: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
    
    # é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„é‚»å±…å’Œåº¦ï¼ˆå‘é‡åŒ–ï¼‰
    node_list = list(G.nodes())
    node_idx_map = {node: idx for idx, node in enumerate(node_list)}
    n_nodes = len(node_list)
    
    # æ„å»ºé‚»æ¥çŸ©é˜µï¼ˆç¨€ç–çŸ©é˜µæ›´å¿«ï¼‰
    adj_matrix = nx.adjacency_matrix(G).astype(float)
    
    # æ‰¹é‡è®¡ç®—èŠ‚ç‚¹åº¦
    degrees = np.array(adj_matrix.sum(axis=1)).flatten()
    degree_dict = {node: degrees[node_idx_map[node]] for node in node_list}
    
    # é¢„è®¡ç®—é‚»å±…é›†åˆï¼ˆä½¿ç”¨ç¨€ç–çŸ©é˜µæ“ä½œï¼‰
    neighbors_dict = {}
    for node in sample_nodes:
        if node in node_idx_map:
            node_idx = node_idx_map[node]
            # è·å–é‚»å±…ç´¢å¼•
            neighbor_indices = adj_matrix[node_idx].nonzero()[1]
            neighbors_dict[node] = set(node_list[i] for i in neighbor_indices)
        else:
            neighbors_dict[node] = set()
    
    # å‡†å¤‡ç»“æœåˆ—è¡¨
    results = []
    
    # æ‰¹é‡è®¡ç®—ç‰¹å¾ï¼ˆé¿å…æ˜¾å¼å¾ªç¯ï¼Œä½¿ç”¨åˆ—è¡¨æ¨å¯¼ï¼‰
    sample_data = window_samples[['node_u', 'node_v']].values
    
    # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰æ ·æœ¬
    for i, (u, v) in enumerate(sample_data):
        idx = window_indices[i]
        
        # è·å–é‚»å±…é›†åˆ
        neighbors_u = neighbors_dict.get(u, set())
        neighbors_v = neighbors_dict.get(v, set())
        
        # ç‰¹å¾1: å…±åŒé‚»å±…æ•°
        common = neighbors_u & neighbors_v
        cn_count = len(common)
        
        # ç‰¹å¾2: Adamic-AdaræŒ‡æ•°
        aa_index = 0
        if cn_count > 0:
            # å‘é‡åŒ–è®¡ç®—
            common_degrees = np.array([degree_dict.get(w, 1) for w in common])
            # é¿å…log(1) = 0çš„æƒ…å†µ
            common_degrees = np.maximum(common_degrees, 2)
            aa_index = np.sum(1.0 / np.log(common_degrees))
        
        # ç‰¹å¾3: èµ„æºåˆ†é…æŒ‡æ•°
        ra_index = 0
        if cn_count > 0:
            # å‘é‡åŒ–è®¡ç®—
            common_degrees = np.array([degree_dict.get(w, 1) for w in common])
            common_degrees = np.maximum(common_degrees, 1)
            ra_index = np.sum(1.0 / common_degrees)
        
        # ç‰¹å¾4: ä¼˜å…ˆè¿æ¥æŒ‡æ•°
        deg_u = degree_dict.get(u, 0)
        deg_v = degree_dict.get(v, 0)
        pa_index = deg_u * deg_v
        
        results.append((idx, cn_count, aa_index, ra_index, pa_index))
    
    return results

# 6. æ‰¹é‡å¤„ç†æ‰€æœ‰çª—å£
print("\n[4] æ‰¹é‡è®¡ç®—X2ç‰¹å¾...")

all_results = []
for idx, row in enumerate(unique_windows.values, 1):
    print(f"\n  çª—å£ {idx}/{len(unique_windows)}: {row[2]}")
    start_time = time.time()
    
    window_results = compute_x2_features_vectorized(row)
    if window_results:
        all_results.extend(window_results)
    
    elapsed = time.time() - start_time
    print(f"    è€—æ—¶: {elapsed:.2f}ç§’")

print(f"\n  å®ŒæˆX2ç‰¹å¾è®¡ç®—ï¼Œå…± {len(all_results)} ä¸ªç»“æœ")

# 7. æ‰¹é‡èµ‹å€¼ç‰¹å¾ï¼ˆå‘é‡åŒ–ï¼‰
print("\n[5] æ‰¹é‡èµ‹å€¼X2ç‰¹å¾...")

if len(all_results) > 0:
    # è½¬æ¢ä¸ºNumPyæ•°ç»„
    results_array = np.array(all_results)
    indices = results_array[:, 0].astype(int)
    
    # æ‰¹é‡èµ‹å€¼
    common_neighbors[indices] = results_array[:, 1]
    adamic_adar[indices] = results_array[:, 2]
    resource_allocation[indices] = results_array[:, 3]
    preferential_attachment[indices] = results_array[:, 4]

# å°†X2ç‰¹å¾æ·»åŠ åˆ°DataFrame
df_x1['common_neighbors'] = common_neighbors
df_x1['adamic_adar'] = adamic_adar
df_x1['resource_allocation'] = resource_allocation
df_x1['preferential_attachment'] = preferential_attachment

# 8. X2ç‰¹å¾æ ‡å‡†åŒ–
print("\n[6] X2ç‰¹å¾æ ‡å‡†åŒ–...")

x2_feature_cols = ['common_neighbors', 'adamic_adar', 'resource_allocation', 'preferential_attachment']

# ä¿å­˜åŸå§‹å€¼
for col in x2_feature_cols:
    df_x1[f'{col}_raw'] = df_x1[col].values

# æ ‡å‡†åŒ–åˆ°[0,1]
scaler = MinMaxScaler()
for col in x2_feature_cols:
    col_values = df_x1[col].values.reshape(-1, 1)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ–¹å·®
    if np.std(col_values) > 1e-10:
        df_x1[col] = scaler.fit_transform(col_values).flatten()
    else:
        print(f"  è­¦å‘Š: {col} æ–¹å·®è¿‡å°ï¼Œä¿æŒåŸå€¼")
        # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œæ ‡å‡†åŒ–ä¸º0æˆ–1
        if np.mean(col_values) > 0:
            df_x1[col] = 1.0
        else:
            df_x1[col] = 0.0

print("  X2ç‰¹å¾å·²æ ‡å‡†åŒ–åˆ° [0, 1] åŒºé—´")

# 9. åˆå¹¶æ‰€æœ‰ç‰¹å¾ï¼ˆX1 + X2ï¼‰
print("\n[7] æ•´ç†æœ€ç»ˆæ•°æ®ï¼ˆX1 + X2ï¼‰...")

# ç¡®å®šè¾“å‡ºåˆ—é¡ºåº
base_cols = [
    'sample_id', 
    'window_t_start', 'window_t_end', 
    'window_t1_start', 'window_t1_end',
    'node_u', 'node_v', 
    'y',
    'window_label',
    'edge_importance',
    'patent_level'
]

# X1ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–åï¼‰
x1_feature_cols = ['link_strength', 'degree_difference', 'betweenness', 'tech_distance']

# æ‰€æœ‰æ ‡å‡†åŒ–åçš„ç‰¹å¾
all_feature_cols = x1_feature_cols + x2_feature_cols

# æ‰€æœ‰åŸå§‹å€¼åˆ—
all_raw_cols = [f'{col}_raw' for col in all_feature_cols]

# ç»„åˆæ‰€æœ‰åˆ—
output_cols = base_cols + all_feature_cols + all_raw_cols

# åªä¿ç•™å­˜åœ¨çš„åˆ—
available_cols = [col for col in output_cols if col in df_x1.columns]
df_final = df_x1[available_cols].copy()

# ç§»é™¤ä¸´æ—¶åˆ—
if 'window_id' in df_final.columns:
    df_final.drop('window_id', axis=1, inplace=True)

# 10. ä¿å­˜ç»“æœ
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"RES03_features_X1X2_complete_{timestamp}.csv"
output_path = os.path.join(DATA_DIR, output_filename)

df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n" + "=" * 80)
print(f"âœ… X1+X2ç‰¹å¾è®¡ç®—å®Œæˆï¼")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
print(f"   è·¯å¾„: {output_path}")
print(f"   æ ·æœ¬æ•°: {len(df_final):,}")
print(f"   ç‰¹å¾æ•°: {len(all_feature_cols)} ä¸ª")
print("=" * 80)

# 11. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
print("\n[8] ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

# å…¨éƒ¨ç‰¹å¾ç»Ÿè®¡
print("\næ‰€æœ‰ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:")
print(df_final[all_feature_cols].describe())

# X2ç‰¹å¾ç»Ÿè®¡
print("\nX2ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:")
print(df_final[x2_feature_cols].describe())

print("\nX2ç‰¹å¾ç»Ÿè®¡ï¼ˆåŸå§‹å€¼ï¼‰:")
x2_raw_cols = [f'{col}_raw' for col in x2_feature_cols]
print(df_final[x2_raw_cols].describe())

# æŒ‰Yå€¼åˆ†ç»„ç»Ÿè®¡
print("\næŒ‰Yå€¼åˆ†ç»„çš„X2ç‰¹å¾å‡å€¼ï¼ˆæ ‡å‡†åŒ–åï¼‰:")
grouped_x2 = df_final.groupby('y')[x2_feature_cols].mean()
print(grouped_x2)

# ç‰¹å¾ç›¸å…³æ€§
print("\næ‰€æœ‰ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ:")
corr_matrix_all = df_final[all_feature_cols].corr()
print(corr_matrix_all)

# 12. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ–‡ä»¶
report_filename = f"RES03_features_X1X2_report_{timestamp}.txt"
report_path = os.path.join(DATA_DIR, report_filename)

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("X1+X2å®Œæ•´ç‰¹å¾è®¡ç®—æŠ¥å‘Š\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"X1æ•°æ®æº: {os.path.basename(X1_FEATURES_FILE)}\n\n")
    
    f.write("æ•°æ®æ¦‚å†µ:\n")
    f.write(f"  æ ·æœ¬æ€»æ•°: {len(df_final):,}\n")
    f.write(f"  æ­£æ ·æœ¬æ•°: {(df_final['y']==1).sum():,}\n")
    f.write(f"  è´Ÿæ ·æœ¬æ•°: {(df_final['y']==0).sum():,}\n")
    f.write(f"  ç‰¹å¾æ€»æ•°: {len(all_feature_cols)}\n")
    f.write(f"    - X1ç‰¹å¾: {len(x1_feature_cols)}ä¸ª\n")
    f.write(f"    - X2ç‰¹å¾: {len(x2_feature_cols)}ä¸ª\n\n")
    
    f.write("X1ç‰¹å¾:\n")
    for i, col in enumerate(x1_feature_cols, 1):
        f.write(f"  {i}. {col}\n")
    
    f.write("\nX2ç‰¹å¾:\n")
    for i, col in enumerate(x2_feature_cols, 1):
        f.write(f"  {i}. {col}\n")
    
    f.write("\næ‰€æœ‰ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:\n")
    f.write(df_final[all_feature_cols].describe().to_string())
    f.write("\n\n")
    
    f.write("X2ç‰¹å¾ç»Ÿè®¡ï¼ˆåŸå§‹å€¼ï¼‰:\n")
    f.write(df_final[x2_raw_cols].describe().to_string())
    f.write("\n\n")
    
    f.write("æŒ‰Yå€¼åˆ†ç»„çš„ç‰¹å¾å‡å€¼å¯¹æ¯”:\n")
    grouped_all = df_final.groupby('y')[all_feature_cols].mean()
    f.write(grouped_all.to_string())
    f.write("\n\n")
    
    # Y=0 vs Y=1 çš„å·®å¼‚åˆ†æ
    f.write("Y=0 vs Y=1 ç‰¹å¾å·®å¼‚åˆ†æ:\n")
    f.write("\nX1ç‰¹å¾å·®å¼‚:\n")
    for col in x1_feature_cols:
        mean_0 = df_final[df_final['y']==0][col].mean()
        mean_1 = df_final[df_final['y']==1][col].mean()
        if mean_0 != 0:
            ratio = mean_1 / mean_0
            diff_pct = (mean_1 - mean_0) / mean_0 * 100
            f.write(f"  {col}: Y=1/Y=0 = {ratio:.3f} (å·®å¼‚: {diff_pct:+.1f}%)\n")
        else:
            f.write(f"  {col}: Y=0å‡å€¼ä¸º0\n")
    
    f.write("\nX2ç‰¹å¾å·®å¼‚:\n")
    for col in x2_feature_cols:
        mean_0 = df_final[df_final['y']==0][col].mean()
        mean_1 = df_final[df_final['y']==1][col].mean()
        if mean_0 != 0:
            ratio = mean_1 / mean_0
            diff_pct = (mean_1 - mean_0) / mean_0 * 100
            f.write(f"  {col}: Y=1/Y=0 = {ratio:.3f} (å·®å¼‚: {diff_pct:+.1f}%)\n")
        else:
            f.write(f"  {col}: Y=0å‡å€¼ä¸º0\n")
    
    f.write("\n")
    
    if 'patent_level' in df_final.columns:
        f.write("æŒ‰ä¸“åˆ©ç­‰çº§çš„X2ç‰¹å¾å‡å€¼:\n")
        patent_x2_stats = df_final.groupby('patent_level')[x2_feature_cols].mean()
        f.write(patent_x2_stats.to_string())
        f.write("\n\n")
    
    f.write("ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µï¼ˆæ‰€æœ‰ç‰¹å¾ï¼‰:\n")
    f.write(corr_matrix_all.to_string())
    f.write("\n\n")
    
    # é«˜ç›¸å…³ç‰¹å¾å¯¹
    f.write("é«˜ç›¸å…³ç‰¹å¾å¯¹ï¼ˆ|r| > 0.7ï¼‰:\n")
    high_corr_pairs = []
    for i in range(len(all_feature_cols)):
        for j in range(i+1, len(all_feature_cols)):
            corr_val = corr_matrix_all.iloc[i, j]
            if abs(corr_val) > 0.7:
                high_corr_pairs.append((all_feature_cols[i], all_feature_cols[j], corr_val))
    
    if high_corr_pairs:
        for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):
            f.write(f"  {feat1} <-> {feat2}: {corr:.3f}\n")
    else:
        f.write("  æ— é«˜ç›¸å…³ç‰¹å¾å¯¹\n")
    
    f.write("\n")
    
    f.write("X2ç‰¹å¾è¯´æ˜:\n")
    f.write("  common_neighbors: å…±åŒé‚»å±…æ•°\n")
    f.write("  adamic_adar: Adamic-AdaræŒ‡æ•°ï¼ˆå…±åŒé‚»å±…çš„åº¦æ•°å€’æ•°å¯¹æ•°å’Œï¼‰\n")
    f.write("  resource_allocation: èµ„æºåˆ†é…æŒ‡æ•°ï¼ˆå…±åŒé‚»å±…çš„åº¦æ•°å€’æ•°å’Œï¼‰\n")
    f.write("  preferential_attachment: ä¼˜å…ˆè¿æ¥æŒ‡æ•°ï¼ˆä¸¤èŠ‚ç‚¹åº¦æ•°çš„ä¹˜ç§¯ï¼‰\n\n")
    
    f.write("ä¼˜åŒ–æŠ€æœ¯:\n")
    f.write("  1. ä½¿ç”¨ç¨€ç–çŸ©é˜µå­˜å‚¨é‚»æ¥å…³ç³»\n")
    f.write("  2. æ‰¹é‡è®¡ç®—èŠ‚ç‚¹åº¦æ•°\n")
    f.write("  3. å‘é‡åŒ–çš„ç‰¹å¾è®¡ç®—\n")
    f.write("  4. NumPyæ•°ç»„æ“ä½œæ›¿ä»£å¾ªç¯\n")
    f.write("  5. é¢„è®¡ç®—å’Œç¼“å­˜é‚»å±…é›†åˆ\n")

print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}")

# 13. éªŒè¯æ•°æ®å®Œæ•´æ€§
print("\n[9] æ•°æ®å®Œæ•´æ€§æ£€æŸ¥...")

# æ£€æŸ¥ç¼ºå¤±å€¼
missing_features = df_final[all_feature_cols].isna().sum()
if missing_features.sum() > 0:
    print("  è­¦å‘Šï¼šå­˜åœ¨ç¼ºå¤±å€¼")
    print(missing_features[missing_features > 0])
else:
    print("  âœ“ æ‰€æœ‰ç‰¹å¾å€¼å®Œæ•´")

# æ£€æŸ¥å¼‚å¸¸å€¼
for col in all_feature_cols:
    outliers = ((df_final[col] < 0) | (df_final[col] > 1)).sum()
    if outliers > 0:
        print(f"  è­¦å‘Š: {col} æœ‰ {outliers} ä¸ªå€¼è¶…å‡º[0,1]èŒƒå›´")

# ç‰¹å¾åˆ†å¸ƒæ£€æŸ¥
print("\nç‰¹å¾å€¼åˆ†å¸ƒæ£€æŸ¥:")
for col in x2_feature_cols:
    unique_vals = df_final[col].nunique()
    if unique_vals < 5:
        print(f"  {col}: åªæœ‰ {unique_vals} ä¸ªä¸åŒå€¼")
        print(f"    å€¼åˆ†å¸ƒ: {df_final[col].value_counts().head()}")

print("\nâœ… X1+X2å®Œæ•´ç‰¹å¾é›†è®¡ç®—å®Œæˆï¼")
print(f"   ä¼˜åŒ–äº®ç‚¹: å‘é‡åŒ–æ“ä½œã€ç¨€ç–çŸ©é˜µã€æ‰¹é‡è®¡ç®—")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
