"""
åŸºæœ¬ç‰¹å¾X1è®¡ç®—è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŸºäºRES01_patent_based_sampledæ•°æ®é›†è®¡ç®—å››ä¸ªåŸºæœ¬ç‰¹å¾
ä½¿ç”¨å‘é‡åŒ–æ“ä½œä¼˜åŒ–è®¡ç®—é€Ÿåº¦
"""

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime
import os
import warnings
from numba import jit
from scipy.sparse import csr_matrix
from sklearn.preprocessing import MinMaxScaler
import multiprocessing as mp
from functools import partial
warnings.filterwarnings('ignore')

# é…ç½®
DATA_DIR = 'F:/WLJ/Weak-Link-Effect-Research/data'

# è¾“å…¥æ–‡ä»¶ - ä½¿ç”¨æ–°çš„åŸºäºä¸“åˆ©æ•°é‡‡æ ·çš„æ•°æ®é›†
BALANCED_DATASET = os.path.join(DATA_DIR, "RES01_patent_based_sampled_20250914_000758.csv")
LINK_STRENGTH_FILE = os.path.join(DATA_DIR, "Keyword_LinkStrength_ByWindow_20250903_212630.csv")

print("=" * 80)
print("åŸºæœ¬ç‰¹å¾X1è®¡ç®—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
print("=" * 80)

# 1. è¯»å–æ•°æ®
print("\n[1] è¯»å–æ•°æ®...")
df_samples = pd.read_csv(BALANCED_DATASET)
df_links_all = pd.read_csv(LINK_STRENGTH_FILE)

print(f"  æ ·æœ¬æ•°: {len(df_samples):,}")
print(f"  å…¨éƒ¨é“¾æ¥æ•°: {len(df_links_all):,}")

# 2. ä¼˜åŒ–æ•°æ®å‡†å¤‡ - ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
print("\n[2] å‡†å¤‡æ•°æ®ï¼ˆå‘é‡åŒ–ï¼‰...")

# åˆ›å»ºè¾¹çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
df_samples['edge_id'] = df_samples['node_u'] + '|' + df_samples['node_v']
df_samples['edge_id_rev'] = df_samples['node_v'] + '|' + df_samples['node_u']

df_links_all['edge_id'] = df_links_all['node_u'] + '|' + df_links_all['node_v']

# åˆ›å»ºæ—¶é—´çª—å£æ ‡è¯†
df_samples['window_id'] = df_samples['window_t_start'].astype(str) + '-' + df_samples['window_t_end'].astype(str)
df_links_all['window_id'] = df_links_all['window_start'].astype(str) + '-' + df_links_all['window_end'].astype(str)

# è·å–æ‰€æœ‰éœ€è¦çš„è¾¹å’Œçª—å£ç»„åˆ
sample_edges = pd.concat([
    df_samples[['window_id', 'edge_id']].rename(columns={'edge_id': 'edge_lookup'}),
    df_samples[['window_id', 'edge_id_rev']].rename(columns={'edge_id_rev': 'edge_lookup'})
]).drop_duplicates()

print(f"  æ ·æœ¬æ¶‰åŠçš„è¾¹-çª—å£ç»„åˆæ•°: {len(sample_edges):,}")

# 3. æ‰¹é‡é¢„å¤„ç†é“¾æ¥æ•°æ®
print("\n[3] é¢„å¤„ç†é“¾æ¥æ•°æ®...")

# åˆ›å»ºé“¾æ¥å¼ºåº¦æŸ¥æ‰¾è¡¨
df_links_all['lookup_key'] = df_links_all['window_id'] + '|' + df_links_all['edge_id']
link_strength_lookup = dict(zip(df_links_all['lookup_key'], df_links_all['link_strength']))

# åå‘è¾¹ä¹ŸåŠ å…¥æŸ¥æ‰¾è¡¨
df_links_all['lookup_key_rev'] = df_links_all['window_id'] + '|' + df_links_all['node_v'] + '|' + df_links_all['node_u']
link_strength_lookup.update(dict(zip(df_links_all['lookup_key_rev'], df_links_all['link_strength'])))

print(f"  é“¾æ¥å¼ºåº¦æŸ¥æ‰¾è¡¨å¤§å°: {len(link_strength_lookup):,}")

# 4. åˆå§‹åŒ–ç‰¹å¾æ•°ç»„ï¼ˆä½¿ç”¨NumPyæ•°ç»„æ›´å¿«ï¼‰
n_samples = len(df_samples)
link_strengths = np.zeros(n_samples)
degree_differences = np.zeros(n_samples)
betweennesses = np.zeros(n_samples)
tech_distances = np.ones(n_samples)  # é»˜è®¤å€¼ä¸º1

# 5. è·å–æ—¶é—´çª—å£
unique_windows = df_samples[['window_t_start', 'window_t_end', 'window_id']].drop_duplicates()
print(f"\n[4] å¤„ç† {len(unique_windows)} ä¸ªæ—¶é—´çª—å£...")

# 6. å®šä¹‰ä¼˜åŒ–çš„ç‰¹å¾è®¡ç®—å‡½æ•°
def compute_window_features(window_data):
    """è®¡ç®—å•ä¸ªæ—¶é—´çª—å£çš„ç‰¹å¾ï¼ˆå¯å¹¶è¡Œï¼‰"""
    w_start, w_end, window_id = window_data
    
    # è·å–è¯¥çª—å£çš„æ ·æœ¬ç´¢å¼•
    window_mask = df_samples['window_id'] == window_id
    window_indices = df_samples[window_mask].index.values
    
    if len(window_indices) == 0:
        return None
    
    # è·å–è¯¥çª—å£çš„æ‰€æœ‰é“¾æ¥
    window_links = df_links_all[df_links_all['window_id'] == window_id]
    
    if len(window_links) == 0:
        return None
    
    # æ„å»ºç½‘ç»œï¼ˆä½¿ç”¨from_pandas_edgelistæ›´å¿«ï¼‰
    G = nx.from_pandas_edgelist(
        window_links, 
        source='node_u', 
        target='node_v', 
        edge_attr='link_strength',
        create_using=nx.Graph()
    )
    
    # ç¡®ä¿æ ·æœ¬èŠ‚ç‚¹éƒ½åœ¨ç½‘ç»œä¸­
    sample_nodes = set(df_samples.loc[window_indices, 'node_u']) | \
                  set(df_samples.loc[window_indices, 'node_v'])
    for node in sample_nodes - set(G.nodes()):
        G.add_node(node)
    
    # æ‰¹é‡è®¡ç®—èŠ‚ç‚¹åº¦
    degrees = dict(G.degree(weight='link_strength'))
    
    # è®¡ç®—è¾¹ä»‹æ•°ï¼ˆå¦‚æœç½‘ç»œä¸å¤ªå¤§ï¼‰
    if G.number_of_edges() < 25000:  # å¯¹äºå¤§ç½‘ç»œï¼Œå¯èƒ½éœ€è¦é‡‡æ ·
        print(G.number_of_edges())
        edge_betweenness = nx.edge_betweenness_centrality(G, weight='link_strength', normalized=True)
        # åˆ›å»ºåŒå‘æŸ¥æ‰¾
        betweenness_lookup = {}
        for (u, v), val in edge_betweenness.items():
            betweenness_lookup[f"{u}|{v}"] = val
            betweenness_lookup[f"{v}|{u}"] = val
    else:
        # å¯¹äºå¤§ç½‘ç»œï¼Œä½¿ç”¨è¿‘ä¼¼ç®—æ³•æˆ–é‡‡æ ·
        betweenness_lookup = {}
    
    # é¢„è®¡ç®—é‚»å±…é›†åˆï¼ˆç”¨äºJaccardè·ç¦»ï¼‰
    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}
    
    # å‡†å¤‡ç»“æœæ•°ç»„
    results = []
    
    # å‘é‡åŒ–è®¡ç®—è¯¥çª—å£æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
    for idx in window_indices:
        u = df_samples.loc[idx, 'node_u']
        v = df_samples.loc[idx, 'node_v']
        edge_key = f"{u}|{v}"
        
        # ç‰¹å¾1: é“¾æ¥å¼ºåº¦
        lookup_key = f"{window_id}|{edge_key}"
        link_str = link_strength_lookup.get(lookup_key, 0)
        
        # ç‰¹å¾2: åº¦å·®
        deg_u = degrees.get(u, 0)
        deg_v = degrees.get(v, 0)
        deg_diff = abs(deg_u - deg_v)
        
        # ç‰¹å¾3: è¾¹ä»‹æ•°
        between = betweenness_lookup.get(edge_key, 0)
        
        # ç‰¹å¾4: Jaccardè·ç¦»ï¼ˆä¼˜åŒ–è®¡ç®—ï¼‰
        if u in neighbors and v in neighbors:
            neigh_u = neighbors[u]
            neigh_v = neighbors[v]
            union_size = len(neigh_u | neigh_v)
            if union_size > 0:
                jaccard_sim = len(neigh_u & neigh_v) / union_size
                tech_dist = 1 - jaccard_sim
            else:
                tech_dist = 1.0
        else:
            tech_dist = 1.0
        
        results.append((idx, link_str, deg_diff, between, tech_dist))
    
    return results

# 7. æ‰¹é‡å¤„ç†æ‰€æœ‰çª—å£ï¼ˆå¯é€‰ï¼šä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼‰
print("\n[5] æ‰¹é‡è®¡ç®—ç‰¹å¾...")

# ä¸²è¡Œå¤„ç†ï¼ˆç¨³å®šä½†è¾ƒæ…¢ï¼‰
all_results = []
for idx, row in enumerate(unique_windows.values, 1):
    print(f"  å¤„ç†çª—å£ {idx}/{len(unique_windows)}: {row[2]}", end='\r')
    window_results = compute_window_features(row)
    if window_results:
        all_results.extend(window_results)

print(f"\n  å®Œæˆç‰¹å¾è®¡ç®—ï¼Œå…± {len(all_results)} ä¸ªç»“æœ")

# 8. æ‰¹é‡èµ‹å€¼ç‰¹å¾ï¼ˆå‘é‡åŒ–ï¼‰
print("\n[6] æ‰¹é‡èµ‹å€¼ç‰¹å¾...")

# å°†ç»“æœè½¬æ¢ä¸ºæ•°ç»„
results_array = np.array(all_results)
if len(results_array) > 0:
    indices = results_array[:, 0].astype(int)
    link_strengths[indices] = results_array[:, 1]
    degree_differences[indices] = results_array[:, 2]
    betweennesses[indices] = results_array[:, 3]
    tech_distances[indices] = results_array[:, 4]

# å°†ç‰¹å¾æ·»åŠ åˆ°DataFrame
df_samples['link_strength'] = link_strengths
df_samples['degree_difference'] = degree_differences
df_samples['betweenness'] = betweennesses
df_samples['tech_distance'] = tech_distances

# 9. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå‘é‡åŒ–ï¼‰
print("\n[7] ç‰¹å¾æ ‡å‡†åŒ–...")

feature_cols = ['link_strength', 'degree_difference', 'betweenness', 'tech_distance']

# ä¿å­˜åŸå§‹å€¼
for col in feature_cols:
    df_samples[f'{col}_raw'] = df_samples[col].values

# ä½¿ç”¨å‘é‡åŒ–çš„æ ‡å‡†åŒ–
scaler = MinMaxScaler()
for col in feature_cols:
    # å¤„ç†ç‰¹æ®Šæƒ…å†µ
    col_values = df_samples[col].values.reshape(-1, 1)
    if np.std(col_values) > 1e-10:  # é¿å…é™¤é›¶
        df_samples[col] = scaler.fit_transform(col_values).flatten()
    else:
        print(f"  è­¦å‘Š: {col} æ–¹å·®è¿‡å°ï¼Œä¿æŒåŸå€¼")

print("  ç‰¹å¾å·²æ ‡å‡†åŒ–åˆ° [0, 1] åŒºé—´")

# 10. æœ€ç»ˆæ•°æ®æ•´ç†
print("\n[8] æ•´ç†æœ€ç»ˆæ•°æ®...")

# ç§»é™¤ä¸´æ—¶åˆ—
df_samples.drop(['edge_id', 'edge_id_rev', 'window_id'], axis=1, inplace=True)

# ç¡®å®šè¾“å‡ºåˆ—é¡ºåº
output_cols = [
    'sample_id', 
    'window_t_start', 'window_t_end', 
    'window_t1_start', 'window_t1_end',
    'node_u', 'node_v', 
    'y',
    'link_strength', 'degree_difference', 'betweenness', 'tech_distance',
    'link_strength_raw', 'degree_difference_raw', 'betweenness_raw', 'tech_distance_raw',
    'window_label',
    'edge_importance',  # ä¿ç•™ä¸“åˆ©æ•°ä¿¡æ¯
    'patent_level'       # ä¿ç•™ä¸“åˆ©ç­‰çº§ä¿¡æ¯
]

# åªä¿ç•™å­˜åœ¨çš„åˆ—
available_cols = [col for col in output_cols if col in df_samples.columns]
df_final = df_samples[available_cols].copy()

# 11. ä¿å­˜ç»“æœ
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"RES02_features_X1_optimized_{timestamp}.csv"
output_path = os.path.join(DATA_DIR, output_filename)

df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n" + "=" * 80)
print(f"âœ… ç‰¹å¾X1è®¡ç®—å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰ï¼")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
print(f"   è·¯å¾„: {output_path}")
print(f"   æ ·æœ¬æ•°: {len(df_final):,}")
print("=" * 80)

# 12. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
print("\n[9] ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

# ç‰¹å¾ç»Ÿè®¡
print("\nç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:")
print(df_final[feature_cols].describe())

print("\nç‰¹å¾ç»Ÿè®¡ï¼ˆåŸå§‹å€¼ï¼‰:")
raw_cols = [f'{col}_raw' for col in feature_cols]
print(df_final[raw_cols].describe())

# æŒ‰Yå€¼åˆ†ç»„ç»Ÿè®¡
print("\næŒ‰Yå€¼åˆ†ç»„çš„ç‰¹å¾å‡å€¼:")
grouped_stats = df_final.groupby('y')[feature_cols].mean()
print(grouped_stats)

# æŒ‰ä¸“åˆ©ç­‰çº§åˆ†ç»„ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if 'patent_level' in df_final.columns:
    print("\næŒ‰ä¸“åˆ©ç­‰çº§åˆ†ç»„çš„ç‰¹å¾å‡å€¼:")
    patent_level_stats = df_final.groupby('patent_level')[feature_cols].mean()
    print(patent_level_stats)

# ç‰¹å¾ç›¸å…³æ€§
print("\nç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ:")
corr_matrix = df_final[feature_cols].corr()
print(corr_matrix)

# 13. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ–‡ä»¶
report_filename = f"RES02_features_X1_report_{timestamp}.txt"
report_path = os.path.join(DATA_DIR, report_filename)

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("åŸºæœ¬ç‰¹å¾X1è®¡ç®—æŠ¥å‘Šï¼ˆä¼˜åŒ–ç‰ˆï¼‰\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"æ•°æ®æº: {os.path.basename(BALANCED_DATASET)}\n\n")
    
    f.write("æ•°æ®æ¦‚å†µ:\n")
    f.write(f"  æ ·æœ¬æ€»æ•°: {len(df_final):,}\n")
    f.write(f"  æ­£æ ·æœ¬æ•°: {(df_final['y']==1).sum():,}\n")
    f.write(f"  è´Ÿæ ·æœ¬æ•°: {(df_final['y']==0).sum():,}\n")
    f.write(f"  æ—¶é—´çª—å£æ•°: {len(unique_windows)}\n\n")
    
    if 'edge_importance' in df_final.columns:
        f.write("ä¸“åˆ©æ•°ç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡ä¸“åˆ©æ•°: {df_final['edge_importance'].mean():.1f}\n")
        f.write(f"  ä¸­ä½æ•°: {df_final['edge_importance'].median():.0f}\n")
        f.write(f"  æœ€å¤§å€¼: {df_final['edge_importance'].max():.0f}\n")
        f.write(f"  æœ€å°å€¼: {df_final['edge_importance'].min():.0f}\n\n")
    
    f.write("ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:\n")
    f.write(df_final[feature_cols].describe().to_string())
    f.write("\n\n")
    
    f.write("ç‰¹å¾ç»Ÿè®¡ï¼ˆåŸå§‹å€¼ï¼‰:\n")
    f.write(df_final[raw_cols].describe().to_string())
    f.write("\n\n")
    
    f.write("æŒ‰Yå€¼åˆ†ç»„çš„ç‰¹å¾å‡å€¼ï¼ˆæ ‡å‡†åŒ–åï¼‰:\n")
    f.write(grouped_stats.to_string())
    f.write("\n\n")
    
    # Y=0 vs Y=1 çš„å·®å¼‚åˆ†æ
    f.write("Y=0 vs Y=1 ç‰¹å¾å·®å¼‚åˆ†æ:\n")
    for col in feature_cols:
        mean_0 = df_final[df_final['y']==0][col].mean()
        mean_1 = df_final[df_final['y']==1][col].mean()
        diff_pct = (mean_1 - mean_0) / mean_0 * 100 if mean_0 != 0 else 0
        f.write(f"  {col}: Y=1/Y=0 = {mean_1/mean_0:.3f} (å·®å¼‚: {diff_pct:+.1f}%)\n")
    f.write("\n")
    
    if 'patent_level' in df_final.columns:
        f.write("æŒ‰ä¸“åˆ©ç­‰çº§åˆ†å¸ƒ:\n")
        patent_dist = df_final.groupby(['patent_level', 'y']).size().unstack(fill_value=0)
        f.write(patent_dist.to_string())
        f.write("\n\n")
    
    f.write("ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ:\n")
    f.write(corr_matrix.to_string())
    f.write("\n\n")
    
    f.write("ä¼˜åŒ–è¯´æ˜:\n")
    f.write("  1. ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯\n")
    f.write("  2. é¢„è®¡ç®—é“¾æ¥å¼ºåº¦æŸ¥æ‰¾è¡¨\n")
    f.write("  3. æ‰¹é‡è®¡ç®—ç‰¹å¾å¹¶ä¸€æ¬¡æ€§èµ‹å€¼\n")
    f.write("  4. ä½¿ç”¨NumPyæ•°ç»„åŠ é€Ÿè®¡ç®—\n")
    f.write("  5. NetworkXçš„from_pandas_edgeliståŠ é€Ÿå»ºå›¾\n\n")
    
    f.write("ç‰¹å¾è¯´æ˜:\n")
    f.write("  link_strength: é“¾æ¥å¼ºåº¦ï¼ˆå…±ç°é¢‘ç‡å½’ä¸€åŒ–ï¼‰\n")
    f.write("  degree_difference: åº¦å·®ï¼ˆèŠ‚ç‚¹åº¦æ•°å·®çš„ç»å¯¹å€¼ï¼‰\n")
    f.write("  betweenness: è¾¹ä»‹æ•°ä¸­å¿ƒæ€§\n")
    f.write("  tech_distance: æŠ€æœ¯è·ç¦»ï¼ˆ1 - Jaccardç›¸ä¼¼åº¦ï¼‰\n")

print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}")

# 14. æ€§èƒ½ç»Ÿè®¡
print("\n[10] æ€§èƒ½ç»Ÿè®¡...")
print(f"  ç‰¹å¾å®Œæ•´æ€§: {(~df_final[feature_cols].isna().any(axis=1)).sum()}/{len(df_final)} ä¸ªæ ·æœ¬ç‰¹å¾å®Œæ•´")

# æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
for col in feature_cols:
    outliers = ((df_final[col] < 0) | (df_final[col] > 1)).sum()
    if outliers > 0:
        print(f"  è­¦å‘Š: {col} æœ‰ {outliers} ä¸ªå€¼è¶…å‡º[0,1]èŒƒå›´")

print("\nâœ… ä¼˜åŒ–ç‰ˆç‰¹å¾è®¡ç®—å®Œæˆï¼")
print(f"   è®¡ç®—é€Ÿåº¦æå‡: é€šè¿‡å‘é‡åŒ–æ“ä½œå’Œæ‰¹é‡å¤„ç†")
print(f"   å†…å­˜ä½¿ç”¨ä¼˜åŒ–: ä½¿ç”¨NumPyæ•°ç»„å’ŒæŸ¥æ‰¾è¡¨")
