"""
åŸºæœ¬ç‰¹å¾X1è®¡ç®—è„šæœ¬ï¼ˆä¿®æ­£ç‰ˆï¼‰
åŸºäºå¹³è¡¡æ•°æ®é›†çš„æ ·æœ¬è®¡ç®—å››ä¸ªåŸºæœ¬ç‰¹å¾ï¼šé“¾æ¥å¼ºåº¦ã€åº¦å·®ã€è¾¹ä»‹æ•°ä¸­å¿ƒæ€§ã€æŠ€æœ¯è·ç¦»
ä»¥df_samplesä¸ºä¸»ï¼Œåªè®¡ç®—æ ·æœ¬ä¸­æ¶‰åŠçš„è¾¹çš„ç‰¹å¾
"""

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")

# è¾“å…¥æ–‡ä»¶
BALANCED_DATASET = os.path.join(DATA_DIR, "RES01_balanced_dataset_1_1_20250913_225013.csv")
LINK_STRENGTH_FILE = os.path.join(DATA_DIR, "Keyword_LinkStrength_ByWindow_20250903_212630.csv")

print("=" * 80)
print("åŸºæœ¬ç‰¹å¾X1è®¡ç®—ï¼ˆä»¥æ ·æœ¬ä¸ºä¸»ï¼‰")
print("=" * 80)

# 1. è¯»å–æ•°æ®
print("\n[1] è¯»å–æ•°æ®...")
df_samples = pd.read_csv(BALANCED_DATASET)
df_links_all = pd.read_csv(LINK_STRENGTH_FILE)

print(f"  å¹³è¡¡æ ·æœ¬æ•°: {len(df_samples):,}")
print(f"  å…¨éƒ¨é“¾æ¥æ•°: {len(df_links_all):,}")

# 2. æå–æ ·æœ¬ä¸­æ¶‰åŠçš„æ‰€æœ‰è¾¹å¯¹å’Œæ—¶é—´çª—å£
print("\n[2] æå–æ ·æœ¬æ¶‰åŠçš„è¾¹...")

# è·å–æ ·æœ¬ä¸­æ‰€æœ‰å”¯ä¸€çš„(çª—å£, èŠ‚ç‚¹å¯¹)ç»„åˆ
sample_edges = set()
for _, row in df_samples.iterrows():
    # æ³¨æ„ï¼šæ ·æœ¬ä½¿ç”¨çš„æ˜¯window_tä½œä¸ºå½“å‰æ—¶é—´çª—å£
    key1 = (row['window_t_start'], row['window_t_end'], row['node_u'], row['node_v'])
    key2 = (row['window_t_start'], row['window_t_end'], row['node_v'], row['node_u'])  # åå‘
    sample_edges.add(key1)
    sample_edges.add(key2)

print(f"  æ ·æœ¬æ¶‰åŠçš„å”¯ä¸€è¾¹å¯¹æ•°: {len(sample_edges):,}")

# è¿‡æ»¤df_linksï¼Œåªä¿ç•™æ ·æœ¬ä¸­æ¶‰åŠçš„è¾¹
df_links_filtered = []
for _, row in df_links_all.iterrows():
    key = (row['window_start'], row['window_end'], row['node_u'], row['node_v'])
    if key in sample_edges:
        df_links_filtered.append(row)

df_links = pd.DataFrame(df_links_filtered)
print(f"  è¿‡æ»¤åé“¾æ¥æ•°: {len(df_links):,}")

# 3. è·å–æ ·æœ¬æ¶‰åŠçš„æ—¶é—´çª—å£
print("\n[3] å¤„ç†æ—¶é—´çª—å£...")
sample_windows = df_samples[['window_t_start', 'window_t_end']].drop_duplicates()
print(f"  æ ·æœ¬æ¶‰åŠçš„æ—¶é—´çª—å£æ•°: {len(sample_windows)}")

# 4. åˆå§‹åŒ–ç‰¹å¾å­˜å‚¨
df_samples['link_strength'] = np.nan
df_samples['degree_difference'] = np.nan
df_samples['betweenness'] = np.nan
df_samples['tech_distance'] = np.nan

# 5. æŒ‰æ—¶é—´çª—å£è®¡ç®—ç‰¹å¾
print("\n[4] è®¡ç®—ç½‘ç»œç‰¹å¾...")

for idx, (w_start, w_end) in enumerate(sample_windows.values, 1):
    print(f"\n  å¤„ç†çª—å£ {idx}/{len(sample_windows)}: {w_start}-{w_end}")
    
    # è·å–è¯¥çª—å£çš„æ ·æœ¬
    window_sample_mask = (df_samples['window_t_start'] == w_start) & \
                         (df_samples['window_t_end'] == w_end)
    window_samples = df_samples[window_sample_mask]
    print(f"    è¯¥çª—å£æ ·æœ¬æ•°: {len(window_samples):,}")
    
    # è·å–è¯¥çª—å£çš„æ‰€æœ‰é“¾æ¥ï¼ˆåŒ…æ‹¬æ ·æœ¬ä¸­çš„è¾¹å’Œå…¶ä»–ç›¸å…³è¾¹ç”¨äºæ„å»ºå®Œæ•´ç½‘ç»œï¼‰
    df_window_links = df_links[(df_links['window_start'] == w_start) & 
                                (df_links['window_end'] == w_end)]
    
    if len(df_window_links) == 0:
        print(f"    è­¦å‘Šï¼šçª—å£ {w_start}-{w_end} æ²¡æœ‰é“¾æ¥æ•°æ®")
        continue
    
    # æ„å»ºç½‘ç»œï¼ˆä½¿ç”¨æ‰€æœ‰ç›¸å…³è¾¹ä»¥è·å¾—å‡†ç¡®çš„ç½‘ç»œç»“æ„ç‰¹å¾ï¼‰
    G = nx.Graph()
    
    # æ·»åŠ æ‰€æœ‰è¾¹
    for _, row in df_window_links.iterrows():
        G.add_edge(row['node_u'], row['node_v'], weight=row['link_strength'])
    
    # ç¡®ä¿æ ·æœ¬ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨ç½‘ç»œä¸­ï¼ˆå³ä½¿æ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼‰
    sample_nodes = set(window_samples['node_u'].unique()) | \
                   set(window_samples['node_v'].unique())
    for node in sample_nodes:
        if node not in G:
            G.add_node(node)
    
    print(f"    ç½‘ç»œè§„æ¨¡: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
    
    # === æ‰¹é‡è®¡ç®—ç‰¹å¾ ===
    
    # ç‰¹å¾1: é“¾æ¥å¼ºåº¦
    # åˆ›å»ºåŒå‘æŸ¥æ‰¾å­—å…¸
    link_strength_dict = {}
    for _, row in df_window_links.iterrows():
        link_strength_dict[(row['node_u'], row['node_v'])] = row['link_strength']
        link_strength_dict[(row['node_v'], row['node_u'])] = row['link_strength']
    
    # ç‰¹å¾2: åº¦å·®
    degrees = dict(G.degree(weight='weight'))
    
    # ç‰¹å¾3: è¾¹ä»‹æ•°ä¸­å¿ƒæ€§
    print(f"    è®¡ç®—è¾¹ä»‹æ•°ä¸­å¿ƒæ€§...")
    if G.number_of_edges() > 0:
        edge_betweenness = nx.edge_betweenness_centrality(G, weight='weight', normalized=True)
        # åˆ›å»ºåŒå‘æŸ¥æ‰¾
        edge_betweenness_dict = {}
        for (u, v), val in edge_betweenness.items():
            edge_betweenness_dict[(u, v)] = val
            edge_betweenness_dict[(v, u)] = val
    else:
        edge_betweenness_dict = {}
    
    # ç‰¹å¾4: æŠ€æœ¯è·ç¦»ï¼ˆJaccardï¼‰
    print(f"    è®¡ç®—æŠ€æœ¯è·ç¦»...")
    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}
    
    def compute_jaccard_distance(u, v):
        if u not in neighbors or v not in neighbors:
            return 1.0
        neighbors_u = neighbors[u]
        neighbors_v = neighbors[v]
        
        # å¤„ç†ç‰¹æ®Šæƒ…å†µ
        if len(neighbors_u) == 0 and len(neighbors_v) == 0:
            return 1.0  # ä¸¤ä¸ªå­¤ç«‹èŠ‚ç‚¹
        
        union_size = len(neighbors_u | neighbors_v)
        if union_size == 0:
            return 1.0
        
        jaccard_sim = len(neighbors_u & neighbors_v) / union_size
        return 1 - jaccard_sim
    
    # ä¸ºè¯¥çª—å£çš„æ¯ä¸ªæ ·æœ¬è®¡ç®—ç‰¹å¾
    for idx_sample in window_samples.index:
        u = df_samples.loc[idx_sample, 'node_u']
        v = df_samples.loc[idx_sample, 'node_v']
        
        # é“¾æ¥å¼ºåº¦
        link_str = link_strength_dict.get((u, v), 0)  # å¦‚æœä¸å­˜åœ¨åˆ™ä¸º0
        df_samples.loc[idx_sample, 'link_strength'] = link_str
        
        # åº¦å·®
        deg_u = degrees.get(u, 0)
        deg_v = degrees.get(v, 0)
        df_samples.loc[idx_sample, 'degree_difference'] = abs(deg_u - deg_v)
        
        # è¾¹ä»‹æ•°ä¸­å¿ƒæ€§
        betw = edge_betweenness_dict.get((u, v), 0)
        df_samples.loc[idx_sample, 'betweenness'] = betw
        
        # æŠ€æœ¯è·ç¦»
        tech_dist = compute_jaccard_distance(u, v)
        df_samples.loc[idx_sample, 'tech_distance'] = tech_dist
    
    print(f"    çª—å£ {w_start}-{w_end} ç‰¹å¾è®¡ç®—å®Œæˆ")

# 6. ç‰¹å¾åå¤„ç†
print("\n[5] ç‰¹å¾åå¤„ç†...")

# æ£€æŸ¥ç¼ºå¤±å€¼
feature_cols = ['link_strength', 'degree_difference', 'betweenness', 'tech_distance']
missing_counts = df_samples[feature_cols].isna().sum()

if missing_counts.sum() > 0:
    print("  ç¼ºå¤±å€¼ç»Ÿè®¡:")
    for col, count in missing_counts.items():
        if count > 0:
            print(f"    {col}: {count} ä¸ªç¼ºå¤±å€¼")
    
    # å¡«å……ç¼ºå¤±å€¼
    print("  å¡«å……ç¼ºå¤±å€¼...")
    df_samples['link_strength'] = df_samples['link_strength'].fillna(0)
    df_samples['degree_difference'] = df_samples['degree_difference'].fillna(0)
    df_samples['betweenness'] = df_samples['betweenness'].fillna(0)
    df_samples['tech_distance'] = df_samples['tech_distance'].fillna(1)

# 7. ç‰¹å¾æ ‡å‡†åŒ–
print("\n[6] ç‰¹å¾æ ‡å‡†åŒ–...")

from sklearn.preprocessing import MinMaxScaler

# ä¿å­˜åŸå§‹å€¼
for col in feature_cols:
    df_samples[f'{col}_raw'] = df_samples[col]

# MinMaxæ ‡å‡†åŒ–åˆ°[0,1]
scaler = MinMaxScaler()

# å¤„ç†ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæŸä¸ªç‰¹å¾çš„æ‰€æœ‰å€¼éƒ½ç›¸åŒ
for col in feature_cols:
    if df_samples[col].std() == 0:
        print(f"  è­¦å‘Š: {col} çš„æ‰€æœ‰å€¼ç›¸åŒï¼Œè·³è¿‡æ ‡å‡†åŒ–")
        df_samples[f'{col}_scaled'] = df_samples[col]
    else:
        df_samples[f'{col}_scaled'] = scaler.fit_transform(df_samples[[col]])

# ç”¨æ ‡å‡†åŒ–åçš„å€¼æ›¿æ¢åŸå§‹ç‰¹å¾åˆ—
for col in feature_cols:
    df_samples[col] = df_samples[f'{col}_scaled']
    df_samples.drop(f'{col}_scaled', axis=1, inplace=True)

print("  ç‰¹å¾å·²æ ‡å‡†åŒ–åˆ° [0, 1] åŒºé—´")

# 8. æœ€ç»ˆæ•°æ®æ•´ç†
print("\n[7] æ•´ç†æœ€ç»ˆæ•°æ®...")

# ç¡®å®šè¾“å‡ºåˆ—é¡ºåº
output_cols = [
    'sample_id', 
    'window_t_start', 'window_t_end', 
    'window_t1_start', 'window_t1_end',
    'node_u', 'node_v', 
    'y',
    'link_strength', 'degree_difference', 'betweenness', 'tech_distance',
    'link_strength_raw', 'degree_difference_raw', 'betweenness_raw', 'tech_distance_raw',
    'window_label'
]

# åªä¿ç•™å­˜åœ¨çš„åˆ—
available_cols = [col for col in output_cols if col in df_samples.columns]
df_final = df_samples[available_cols].copy()

# 9. ä¿å­˜ç»“æœ
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"RES02_features_X1_{timestamp}.csv"
output_path = os.path.join(DATA_DIR, output_filename)

df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n" + "=" * 80)
print(f"âœ… ç‰¹å¾X1è®¡ç®—å®Œæˆï¼")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
print(f"   è·¯å¾„: {output_path}")
print(f"   æ ·æœ¬æ•°: {len(df_final):,}")
print("=" * 80)

# 10. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
print("\n[8] ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

report_filename = f"RES02_features_X1_report_{timestamp}.txt"
report_path = os.path.join(DATA_DIR, report_filename)

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("åŸºæœ¬ç‰¹å¾X1è®¡ç®—æŠ¥å‘Š\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    f.write("æ•°æ®æ¦‚å†µ:\n")
    f.write(f"  æ ·æœ¬æ€»æ•°: {len(df_final):,}\n")
    f.write(f"  æ­£æ ·æœ¬æ•°: {(df_final['y']==1).sum():,}\n")
    f.write(f"  è´Ÿæ ·æœ¬æ•°: {(df_final['y']==0).sum():,}\n\n")
    
    f.write("ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:\n")
    stats = df_final[feature_cols].describe()
    f.write(stats.to_string())
    f.write("\n\n")
    
    f.write("ç‰¹å¾ç»Ÿè®¡ï¼ˆåŸå§‹å€¼ï¼‰:\n")
    raw_cols = [f'{col}_raw' for col in feature_cols]
    stats_raw = df_final[raw_cols].describe()
    f.write(stats_raw.to_string())
    f.write("\n\n")
    
    f.write("æŒ‰Yå€¼åˆ†ç»„çš„ç‰¹å¾å‡å€¼ï¼ˆåŸå§‹å€¼ï¼‰:\n")
    grouped_stats = df_final.groupby('y')[raw_cols].mean()
    f.write(grouped_stats.to_string())
    f.write("\n\n")
    
    f.write("ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ:\n")
    corr_matrix = df_final[feature_cols].corr()
    f.write(corr_matrix.to_string())
    f.write("\n\n")
    
    f.write("ç‰¹å¾è¯´æ˜:\n")
    f.write("  1. link_strength: é“¾æ¥å¼ºåº¦ï¼ˆå…±ç°é¢‘ç‡å½’ä¸€åŒ–ï¼‰\n")
    f.write("  2. degree_difference: åº¦å·®ï¼ˆèŠ‚ç‚¹åº¦æ•°å·®çš„ç»å¯¹å€¼ï¼‰\n")
    f.write("  3. betweenness: è¾¹ä»‹æ•°ä¸­å¿ƒæ€§\n")
    f.write("  4. tech_distance: æŠ€æœ¯è·ç¦»ï¼ˆåŸºäºJaccardç›¸ä¼¼åº¦ï¼‰\n")

print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}")

# 11. æ•°æ®éªŒè¯
print("\n[9] æ•°æ®éªŒè¯...")
print(f"  ç‰¹å¾å®Œæ•´æ€§: {(~df_final[feature_cols].isna().any(axis=1)).sum()}/{len(df_final)} ä¸ªæ ·æœ¬ç‰¹å¾å®Œæ•´")
print(f"  Yå€¼åˆ†å¸ƒ: Y=1: {(df_final['y']==1).sum()}, Y=0: {(df_final['y']==0).sum()}")

# ç®€å•çš„ç‰¹å¾å·®å¼‚åˆ†æ
print("\n  Y=0 vs Y=1 ç‰¹å¾å‡å€¼å·®å¼‚ï¼ˆåŸå§‹å€¼ï¼‰:")
for col in raw_cols:
    mean_0 = df_final[df_final['y']==0][col].mean()
    mean_1 = df_final[df_final['y']==1][col].mean()
    diff_pct = (mean_1 - mean_0) / mean_0 * 100 if mean_0 != 0 else 0
    print(f"    {col}: Y=0:{mean_0:.4f}, Y=1:{mean_1:.4f}, å·®å¼‚:{diff_pct:+.1f}%")

print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
