"""
åŸºæœ¬ç‰¹å¾X1è®¡ç®—è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŸºäºå¹³è¡¡æ•°æ®é›†è®¡ç®—å››ä¸ªåŸºæœ¬ç‰¹å¾ï¼šé“¾æ¥å¼ºåº¦ã€åº¦å·®ã€è¾¹ä»‹æ•°ä¸­å¿ƒæ€§ã€æŠ€æœ¯è·ç¦»
é‡‡ç”¨æ‰¹é‡è®¡ç®—å’Œå‘é‡åŒ–æ“ä½œï¼Œé¿å…å¾ªç¯ï¼Œæé«˜è®¡ç®—æ•ˆç‡
"""

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
DATA_DIR = 'F:/WLJ/Weak-Link-Effect-Research/data'

# è¾“å…¥æ–‡ä»¶
BALANCED_DATASET = os.path.join(DATA_DIR, "RES01_balanced_dataset_1_1_20250913_225013.csv")
LINK_STRENGTH_FILE = os.path.join(DATA_DIR, "Keyword_LinkStrength_ByWindow_20250903_212630.csv")

print("=" * 80)
print("åŸºæœ¬ç‰¹å¾X1è®¡ç®—")
print("=" * 80)

# 1. è¯»å–æ•°æ®
print("\n[1] è¯»å–æ•°æ®...")
df_samples = pd.read_csv(BALANCED_DATASET)
df_links = pd.read_csv(LINK_STRENGTH_FILE)

print(f"  æ ·æœ¬æ•°: {len(df_samples):,}")
print(f"  é“¾æ¥æ•°: {len(df_links):,}")

# 2. é¢„è®¡ç®—æ‰€æœ‰æ—¶é—´çª—å£çš„ç½‘ç»œç‰¹å¾
print("\n[2] æ‰¹é‡è®¡ç®—ç½‘ç»œç‰¹å¾...")

# è·å–æ‰€æœ‰å”¯ä¸€çš„æ—¶é—´çª—å£
windows = df_links[['window_start', 'window_end']].drop_duplicates()
print(f"  æ—¶é—´çª—å£æ•°: {len(windows)}")

# å­˜å‚¨æ‰€æœ‰çª—å£çš„ç‰¹å¾
all_features = []

for idx, (w_start, w_end) in enumerate(windows.values, 1):
    print(f"\n  å¤„ç†çª—å£ {idx}/{len(windows)}: {w_start}-{w_end}")
    
    # è·å–è¯¥çª—å£çš„æ‰€æœ‰è¾¹
    df_window = df_links[(df_links['window_start'] == w_start) & 
                         (df_links['window_end'] == w_end)]
    
    # æ„å»ºç½‘ç»œ
    G = nx.Graph()
    edge_list = df_window[['node_u', 'node_v', 'link_strength']].values
    G.add_weighted_edges_from(edge_list, weight='weight')
    
    print(f"    èŠ‚ç‚¹æ•°: {G.number_of_nodes()}, è¾¹æ•°: {G.number_of_edges()}")
    
    # === æ‰¹é‡è®¡ç®—ç‰¹å¾ ===
    
    # ç‰¹å¾1: é“¾æ¥å¼ºåº¦ï¼ˆç›´æ¥ä»df_windowè·å–ï¼‰
    link_strengths = df_window.set_index(['node_u', 'node_v'])['link_strength'].to_dict()
    
    # ç‰¹å¾2: åº¦å·®ï¼ˆå‘é‡åŒ–è®¡ç®—ï¼‰
    degrees = dict(G.degree(weight='weight'))
    df_window['degree_u'] = df_window['node_u'].map(degrees)
    df_window['degree_v'] = df_window['node_v'].map(degrees)
    df_window['degree_difference'] = np.abs(df_window['degree_u'] - df_window['degree_v'])
    
    # ç‰¹å¾3: è¾¹ä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆæ‰¹é‡è®¡ç®—ï¼‰
    print(f"    è®¡ç®—è¾¹ä»‹æ•°ä¸­å¿ƒæ€§...")
    edge_betweenness = nx.edge_betweenness_centrality(G, weight='weight', normalized=True)
    # å¤„ç†åŒå‘è¾¹
    edge_betweenness_full = {}
    for (u, v), val in edge_betweenness.items():
        edge_betweenness_full[(u, v)] = val
        edge_betweenness_full[(v, u)] = val
    
    df_window['betweenness'] = df_window.apply(
        lambda row: edge_betweenness_full.get((row['node_u'], row['node_v']), 0), 
        axis=1
    )
    
    # ç‰¹å¾4: æŠ€æœ¯è·ç¦»ï¼ˆä½¿ç”¨Jaccardï¼Œå‘é‡åŒ–è®¡ç®—ï¼‰
    print(f"    è®¡ç®—æŠ€æœ¯è·ç¦»...")
    
    # é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„é‚»å±…é›†åˆ
    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}
    
    def compute_jaccard_distance(row):
        u, v = row['node_u'], row['node_v']
        if u not in neighbors or v not in neighbors:
            return 1.0  # æœ€å¤§è·ç¦»
        neighbors_u = neighbors[u]
        neighbors_v = neighbors[v]
        union_size = len(neighbors_u | neighbors_v)
        if union_size == 0:
            return 1.0
        jaccard_sim = len(neighbors_u & neighbors_v) / union_size
        return 1 - jaccard_sim
    
    df_window['tech_distance'] = df_window.apply(compute_jaccard_distance, axis=1)
    
    # ä¿ç•™éœ€è¦çš„ç‰¹å¾åˆ—
    df_window_features = df_window[['window_start', 'window_end', 'node_u', 'node_v',
                                    'link_strength', 'degree_difference', 
                                    'betweenness', 'tech_distance']]
    
    all_features.append(df_window_features)
    print(f"    ç‰¹å¾è®¡ç®—å®Œæˆ")

# åˆå¹¶æ‰€æœ‰çª—å£çš„ç‰¹å¾
print("\n[3] åˆå¹¶ç‰¹å¾æ•°æ®...")
df_features = pd.concat(all_features, ignore_index=True)
print(f"  æ€»ç‰¹å¾è®°å½•æ•°: {len(df_features):,}")

# 3. å°†ç‰¹å¾ä¸æ ·æœ¬æ•°æ®å…³è”
print("\n[4] ç‰¹å¾ä¸æ ·æœ¬å…³è”...")

# ä¸ºäº†mergeï¼Œéœ€è¦ç¡®ä¿çª—å£å’ŒèŠ‚ç‚¹å¯¹çš„åŒ¹é…
# æ³¨æ„ï¼šæ ·æœ¬ä¸­çš„window_tå¯¹åº”ç‰¹å¾ä¸­çš„window
df_samples_merge = df_samples.copy()
df_features_merge = df_features.copy()

# é‡å‘½ååˆ—ä»¥ä¾¿merge
df_features_merge = df_features_merge.rename(columns={
    'window_start': 'window_t_start',
    'window_end': 'window_t_end'
})

# æ‰§è¡Œmergeï¼ˆå·¦è¿æ¥ï¼Œä¿ç•™æ‰€æœ‰æ ·æœ¬ï¼‰
df_result = pd.merge(
    df_samples_merge,
    df_features_merge,
    on=['window_t_start', 'window_t_end', 'node_u', 'node_v'],
    how='left'
)

# æ£€æŸ¥æ˜¯å¦æœ‰æœªåŒ¹é…çš„æ ·æœ¬ï¼ˆå¯èƒ½æ˜¯è¾¹æ–¹å‘é—®é¢˜ï¼‰
unmatched_mask = df_result['link_strength'].isna()
if unmatched_mask.any():
    print(f"  å‘ç° {unmatched_mask.sum()} ä¸ªæœªåŒ¹é…æ ·æœ¬ï¼Œå°è¯•åå‘åŒ¹é…...")
    
    # åˆ›å»ºåå‘ç‰¹å¾è¡¨ï¼ˆäº¤æ¢uå’Œvï¼‰
    df_features_reverse = df_features_merge.copy()
    df_features_reverse[['node_u', 'node_v']] = df_features_reverse[['node_v', 'node_u']]
    
    # å¯¹æœªåŒ¹é…çš„æ ·æœ¬è¿›è¡Œåå‘merge
    df_unmatched = df_samples_merge[unmatched_mask].copy()
    df_unmatched_merged = pd.merge(
        df_unmatched,
        df_features_reverse,
        on=['window_t_start', 'window_t_end', 'node_u', 'node_v'],
        how='left'
    )
    
    # æ›´æ–°ç»“æœ
    df_result.loc[unmatched_mask] = df_unmatched_merged

# 4. ç‰¹å¾åå¤„ç†
print("\n[5] ç‰¹å¾åå¤„ç†...")

# æ£€æŸ¥ç¼ºå¤±å€¼
missing_counts = df_result[['link_strength', 'degree_difference', 
                            'betweenness', 'tech_distance']].isna().sum()
if missing_counts.sum() > 0:
    print("  ç¼ºå¤±å€¼ç»Ÿè®¡:")
    for col, count in missing_counts.items():
        if count > 0:
            print(f"    {col}: {count} ä¸ªç¼ºå¤±å€¼")
    
    # å¡«å……ç¼ºå¤±å€¼ï¼ˆå¦‚æœè¾¹åœ¨tæ—¶åˆ»ä¸å­˜åœ¨ï¼Œè¯´æ˜æ˜¯æ–°å‡ºç°çš„å¼±é“¾æ¥ï¼‰
    df_result['link_strength'] = df_result['link_strength'].fillna(0)
    df_result['degree_difference'] = df_result['degree_difference'].fillna(0)
    df_result['betweenness'] = df_result['betweenness'].fillna(0)
    df_result['tech_distance'] = df_result['tech_distance'].fillna(1)  # æœ€å¤§è·ç¦»

# 5. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
print("\n[6] ç‰¹å¾æ ‡å‡†åŒ–...")

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# é€‰æ‹©éœ€è¦æ ‡å‡†åŒ–çš„ç‰¹å¾
feature_cols = ['link_strength', 'degree_difference', 'betweenness', 'tech_distance']

# ä¿å­˜åŸå§‹å€¼
for col in feature_cols:
    df_result[f'{col}_raw'] = df_result[col]

# ä½¿ç”¨MinMaxæ ‡å‡†åŒ–åˆ°[0,1]
scaler = MinMaxScaler()
df_result[feature_cols] = scaler.fit_transform(df_result[feature_cols])

print("  ç‰¹å¾å·²æ ‡å‡†åŒ–åˆ° [0, 1] åŒºé—´")

# 6. æœ€ç»ˆæ•°æ®æ•´ç†
print("\n[7] æ•´ç†æœ€ç»ˆæ•°æ®...")

# é‡æ–°æ’åºåˆ—
final_cols = [
    'sample_id', 'window_t_start', 'window_t_end', 
    'window_t1_start', 'window_t1_end',
    'node_u', 'node_v', 'y',
    'link_strength', 'degree_difference', 'betweenness', 'tech_distance',
    'link_strength_raw', 'degree_difference_raw', 'betweenness_raw', 'tech_distance_raw',
    'window_label'
]

# ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
for col in final_cols:
    if col not in df_result.columns and col != 'window_label':
        print(f"  è­¦å‘Š: åˆ— {col} ä¸å­˜åœ¨")

df_final = df_result[[col for col in final_cols if col in df_result.columns]]

# 7. ä¿å­˜ç»“æœ
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"RES02_features_X1_{timestamp}.csv"
output_path = os.path.join(DATA_DIR, output_filename)

df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

print("\n" + "=" * 80)
print(f"âœ… ç‰¹å¾X1è®¡ç®—å®Œæˆï¼")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
print(f"   è·¯å¾„: {output_path}")
print(f"   æ ·æœ¬æ•°: {len(df_final):,}")
print("=" * 80)

# 8. ç”Ÿæˆç‰¹å¾ç»Ÿè®¡æŠ¥å‘Š
print("\n[8] ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

report_filename = f"RES02_features_X1_report_{timestamp}.txt"
report_path = os.path.join(DATA_DIR, report_filename)

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("åŸºæœ¬ç‰¹å¾X1è®¡ç®—æŠ¥å‘Š\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    f.write("æ•°æ®æ¦‚å†µ:\n")
    f.write(f"  æ ·æœ¬æ€»æ•°: {len(df_final):,}\n")
    f.write(f"  æ­£æ ·æœ¬æ•°: {(df_final['y']==1).sum():,}\n")
    f.write(f"  è´Ÿæ ·æœ¬æ•°: {(df_final['y']==0).sum():,}\n\n")
    
    f.write("ç‰¹å¾ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:\n")
    stats = df_final[feature_cols].describe()
    f.write(stats.to_string())
    f.write("\n\n")
    
    f.write("ç‰¹å¾ç»Ÿè®¡ï¼ˆåŸå§‹å€¼ï¼‰:\n")
    raw_cols = [f'{col}_raw' for col in feature_cols]
    stats_raw = df_final[raw_cols].describe()
    f.write(stats_raw.to_string())
    f.write("\n\n")
    
    f.write("ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ:\n")
    corr_matrix = df_final[feature_cols].corr()
    f.write(corr_matrix.to_string())
    f.write("\n\n")
    
    f.write("ç‰¹å¾è¯´æ˜:\n")
    f.write("  1. link_strength: é“¾æ¥å¼ºåº¦ï¼ˆå…±ç°é¢‘ç‡å½’ä¸€åŒ–ï¼‰\n")
    f.write("  2. degree_difference: åº¦å·®ï¼ˆèŠ‚ç‚¹åº¦æ•°å·®çš„ç»å¯¹å€¼ï¼‰\n")
    f.write("  3. betweenness: è¾¹ä»‹æ•°ä¸­å¿ƒæ€§\n")
    f.write("  4. tech_distance: æŠ€æœ¯è·ç¦»ï¼ˆåŸºäºJaccardç›¸ä¼¼åº¦ï¼‰\n")

print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}")

# 9. å¿«é€ŸéªŒè¯
print("\n[9] æ•°æ®éªŒè¯...")
print(f"  ç‰¹å¾å®Œæ•´æ€§: {(~df_final[feature_cols].isna().any(axis=1)).sum()}/{len(df_final)} ä¸ªæ ·æœ¬ç‰¹å¾å®Œæ•´")
print(f"  Yå€¼åˆ†å¸ƒ: Y=1: {(df_final['y']==1).sum()}, Y=0: {(df_final['y']==0).sum()}")

print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
